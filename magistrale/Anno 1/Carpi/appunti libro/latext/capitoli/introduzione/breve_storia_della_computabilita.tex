\subsection{Breve Storia della computabilità}

Questo esame si interessa anzitutto del tema della computabilità: parola forse impegnativa e difficile, e pur tuttavia ovviamente collegata all'informatica, capace di
richiamare l'idea del computer e del calcolatore.
Vale allora la pena di spiegarne subito la rilevanza, anche per introdurre in maniera adeguata i temi della prima parte del corso.
Nella accezione originaria e comune, computare, o calcolare, significa far di conto con i numeri usuali, e cioè gli interi $0, \pm 1, \pm 2, ...$:
sommarli, moltiplicarli, semmai sottrarli e dividerli. Per chi ha qualche maggior dimestichezza con un pò
di matematica, l'arte di far di conto si può estendere ad operazioni più sofisticate e a contesti più ampi (limiti, derivate, integrali). Ma, in realtà, tutti i problemi
che si prestano a formalizzazioni tramite modelli matematici possono essere "computati".
Già René Descartes (Cartesio), secoli fa, all'inizio del Seicento, vagheggiava la possibilità di tradurre in termini matematici ogni problema, e dunque di risolverlo
tramite equazioni e computazioni; applicava poi l'intuizione al campo geometrico, identificando, ad esempio, punti del piano con coppie ordinate
di numeri (reali), rette del piano con equazioni di primo grado in due incognite, e così via.
Pochi anni dopo, nel 1666, Gottfried Wilhelm Leibniz esortava appassionatamente "Calculemus" (calcoliamo!), invitando ad usare conti e computazioni
anche per risolvere oggettivamente gli stessi conflitti umani e condurre a termine le controversie trasferendole "dai ragionamenti complicati ai calcoli semplici, dai
vocaboli di significato incerto e vago a caratteri determinati". Del resto lo stesso Leibniz provvedeva a chiarire di intendere per calcolo
"qualunque notazione che rappresenti il ragionamento, quand'anche non avesse alcun rapporto con i numeri ".
A queste applicazioni computazionali, rivolte ad orizzonti sempre più larghi, si accompagnava il tentativo di costruire macchine calcolatrici capaci di aiutare,
simulare e, talora, sostituire, la mente umana nel suo lavoro. Proprio al Seicento risalgono vari esempi di meccanismi automatici per fare di conto e svolgere almeno
le tradizionali operazioni di somma, prodotto, sottrazione e divisione. Possiamo così ricordare che proprio Leibniz aveva concepito nel 1671 una ruota adatta
a sviluppare questo genere di computazione, preceduto comunque in questo da Blaise Pascal e da Wilhelm Schickard che, rispettivamente nel 1643 e nel 1623,
avevano inventato analoghi procedimenti meccanici. Nei secoli successivi ci furono anche casi di macchine capaci di svolgere computazioni fuori dal solo ambito
numerico. Si parla, ad esempio, di un automa di Kempelen capace di giocare a scacchi (1768) e di anticipare in questo senso il moderno Deep Blue (il computer IBM capace di sconfiggere
qualche anno fa anche il campione del mondo degli scacchi): ma, nel caso di Kempelen, non fu mai chiaro quale fosse il segreto programma della macchina e ci fu chi
fondatamente dubitò trattarsi soltanto di un imbroglio ben riuscito (i lettori appassionati di libri polizieschi potranno trovare ampia discussione dell' argomento
nel giallo "L'Automa" di J. Dickson Carr). Nell'Ottocento, tuttavia, Charles Babbage concepì teoricamente un meccanismo automatico, da lui chiamato macchina analitica (analytical engine),
virtualmente capace di adattarsi non solo a numeri e a mosse di scacchi, ma ad ogni possibile contesto, dunque una sorta di hardware universale, che Ada Lovelace Byron
si preoccupò in quegli stessi anni di corredare di quel che oggi chiameremmo il software.
A questi progressi tecnici dovevano del resto accompagnarsi corrispondenti approfondimenti teorici,
a proposito di una questione che diventa fondamentale non appena si affronta il tema del
rapporto uomo-macchina: si tratta infatti di

\begin{itemize}
    \item identificare un linguaggio astratto appropriato, con simboli opportuni, in cui
          formulare i problemi (matematici e non) da risolvere, per poterli poi proporre
          alla macchina che deve computarli,
    \item individuare, ancora, le leggi che la macchina deve seguire per svolgere i suoi
          calcoli.
\end{itemize}

Entrambe le questioni sono tutto men che trascurabili. Oggi, ad esempio, siamo
abituati ad usare le cifre $0, 1, 2, 3, 4, 5, 6, 7, 8, 9$ per rappresentare i numeri interi,
e i simboli $+, \cdot , -$, : per indicare le loro usuali operazioni. Ma la genesi di questi
simboli non è stato processo banale ed ha richiesto talora il progresso di secoli;
del resto, gli antichi Greci e Romani non utilizzavano ancora questi caratteri, ma
altri ben più complicati. Ancora nel Rinascimento, o nello stesso Seicento, queste notazioni si affacciavano timidamente.
Anzi, proprio a Leibniz si attribuisce l'idea di un linguaggio universale
("lingua characteristica" nella denominazione latina da lui adoperata) con cui formulare le comunicazioni scientifiche
tra uomini di idiomi diversi, e dunque anche tra uomo e macchina.
Sempre Leibniz propugnò l'idea di un calcolo della ragione ("calculus ratiocinator" in latino) atto ad individuare
le leggi fondamentali di sviluppo del pensiero (e delle computazioni), utile dunque anche nella programmazione di nuove
macchine pensanti. Come si vede, siamo qui agli albori di quelle che oggi si chiamano
intelligenza artificiale e deduzione automatica, della capacità, cioè, di sviluppare calcoli e pensieri in modo meccanico,
dunque delegabile ai "computers". Del resto, l'idea di uno studio dei procedimenti del pensiero
(quello che in termini ufficiali si chiama Logica) è ben precedente Leibniz, e risale almeno ad Aristotele e
al 300 avanti Cristo. Il calculus ratiocinator fu comunque ampiamente sviluppato
già nell'Ottocento da George Boole, che formulò una sorta di teoria algebrica del
pensiero, oggi comunemente conosciuta proprio come algebra Booleana.
Come si vede, il tema della computabilità si collega a svariatissimi argomenti
come

\begin{itemize}
    \item le macchine calcolatrici per attuare le computazioni,
    \item i linguaggi e le regole con cui favorire la loro collaborazione,
\end{itemize}

ed altri ancora. Tra l'altro, è interessante osservare come queste riflessioni anticipino largamente la nascita
dei moderni computers e si sviluppino nei secoli precedenti. Infatti i primi calcolatori
(come oggi comunemente li intendiamo) risalgono a tempi relativamente recenti, al periodo della seconda guerra mondiale e
agli anni immediatamente successivi se è vero, come è vero, che il primo computer elettronico, l'ENIAC
(architettato da Von Neumann) è del 1946.
La computabilità che vogliamo qui trattare si interessa ovviamente a tutte queste
problematiche, ma vuole anche sottolinearne un aspetto ulteriore. In effetti, il suo
ideale punto di partenza e, se vogliamo, l'anno stesso di nascita della moderna
Informatica Teorica (se pure ha senso istituire un'anagrafe delle correnti scientifiche) si colloca in una
data intermedia, che segue tutti i germi computazionali che abbiamo descritto,
ma precede di una decina di anni la nascita di ENIAC. Infatti,
la data che attira il nostro interesse è il 1936. Nel prossimo paragrafo tenteremo
di spiegare il perché.