# Machine Learning

## Indice

* [Definizione di un problema di learning](#definizione-di-un-problema-di-learning)
* [Quando usare il Machine Learning](#quando-usare-il-machine-learning)
* [Tipi di Learning](#tipi-di-learning)
  + [Supervised Learning](#supervised-learning)
* [Progettare un sistema di Learning](#progettare-un-sistema-di-learning)
* [Classificazione](#classificazione)
  + [Decision Treeee :evergreen_tree:](#decision-treeee--evergreen-tree-)
  + [Hunt's Algorithm](#hunt-s-algorithm)
* [Problemi di design di alberi ad induzione elettromagnetica :zap:](#problemi-di-design-di-alberi-ad-induzione-elettromagnetica--zap-)
  + [Metodi per effettuare i test](#metodi-per-effettuare-i-test)
    - [Gini](#gini)
    - [Entropy](#entropy)
    - [Pro Vs Cons](#pro-vs-cons)
  + [Caratteristiche degli Alberi](#caratteristiche-degli-alberi)
* [Errori](#errori)
  + [Overfitting e Underfitting](#overfitting-e-underfitting)
* [Model Selection](#model-selection)
  + [Approccio pessimistico](#approccio-pessimistico)
  + [Approccio Ottimistico](#approccio-ottimistico)
  + [PrePruning](#prepruning)
  + [PostPruning](#postpruning)
* [Valutazione delle Performance di un Classificacatore](#valutazione-delle-performance-di-un-classificacatore)
* [Imbalanced Class Problem ‚öñÔ∏è](#imbalanced-class-problem---)
  + [Precision & Recall](#precision---recall)
  + [ROC ü™®](#roc---)
    - [Generazione di una ROC](#generazione-di-una-roc)
  + [Conclusione del problema](#conclusione-del-problema)
* [Nearest Neighbor Classification](#nearest-neighbor-classification)
  + [Vantaggi](#vantaggi)
  + [Svantaggi](#svantaggi)
* [Bayesian Classification](#bayesian-classification)
  + [Naive Bayesian Classifier](#naive-bayesian-classifier)
    - [In Breve](#in-breve)
  + [Bayesian Belief Network](#bayesian-belief-network)
    - [In Breve](#in-breve-1)
* [Support Vector Machine üé∞](#support-vector-machine---)
  + [Classificazione](#classificazione-1)
  + [Training - Caso Separabile](#training---caso-separabile)
  + [Training - Caso non Separabile](#training---caso-non-separabile)
  + [Training - Caso Non Lineare](#training---caso-non-lineare)
  + [Caratteristiche](#caratteristiche)
* [Ensemble Methods](#ensemble-methods)
  + [Metodi per costruire Ensemble Classifier](#metodi-per-costruire-ensemble-classifier)
  + [Bias-Variance Decomposition](#bias-variance-decomposition)
  + [Bagging](#bagging)
  + [Boosting](#boosting)
    - [AdaBoosting](#adaboosting)
  + [Random Forest](#random-forest)
* [Artificial Neural Network (ANN)](#artificial-neural-network--ann-)
  + [Percettrone (pompotron :robot:)](#percettrone--pompotron--robot--)
    - [Modello di Apprendimento del Pompotron](#modello-di-apprendimento-del-pompotron)
  + [Esempi di Funzioni di Attivazione](#esempi-di-funzioni-di-attivazione)
  + [MultyLayer ANN](#multylayer-ann)
  + [Learning per ANN](#learning-per-ann)
  + [Convolutional Neural Network](#convolutional-neural-network)
  + [Problemi di Design delle ANN](#problemi-di-design-delle-ann)
- [Cluster Analysis](#cluster-analysis)
  * [Tipi di clustering](#tipi-di-clustering)
    + [Hierarchical vs Partitional](#hierarchical-vs-partitional)
    + [Exclusive vs Overlapping vs Fuzzy](#exclusive-vs-overlapping-vs-fuzzy)
    + [Complete vs Partial](#complete-vs-partial)
  * [Tipi di Cluster](#tipi-di-cluster)
  * [Clustering Algorithms](#clustering-algorithms)
    + [K-means](#k-means)
      - [Algoritmo](#algoritmo)
      - [Costo](#costo)
      - [Bisecting K-Means](#bisecting-k-means)
      - [Applicabilit√†](#applicabilit-)
    + [Agglomerative Hierarchical](#agglomerative-hierarchical)
      - [Basic algorithm](#basic-algorithm)
      - [Complessit√†](#complessit-)
      - [Forza e Punti Deboli](#forza-e-punti-deboli)
    + [DBScan](#dbscan)
      - [Algoritmo](#algoritmo-1)
      - [Complessit√† in Spazio e Tempo](#complessit--in-spazio-e-tempo)
      - [Vantaggi e Svantaggi](#vantaggi-e-svantaggi)
  * [Cluster Evaluation](#cluster-evaluation)
    + [Unsupervised Cohesion and Separation](#unsupervised-cohesion-and-separation)
      - [Graph Based](#graph-based)
      - [Prototype Based](#prototype-based)
      - [Overall](#overall)
      - [Relazione tra Coesione e Separation](#relazione-tra-coesione-e-separation)
      - [Silhouette Coefficient](#silhouette-coefficient)
      - [Unsupervised Similarity Matrix](#unsupervised-similarity-matrix)
      - [Giusto numero di cluster](#giusto-numero-di-cluster)
    + [Supervised Measures](#supervised-measures)
    + [Assessing the Significance of Cluster Validity Measures](#assessing-the-significance-of-cluster-validity-measures)
- [Anomaly Detection](#anomaly-detection)
  * [Cause delle Anomalie](#cause-delle-anomalie)
  * [Differenti Approcci](#differenti-approcci)
    + [Approccio Statistico](#approccio-statistico)
      - [Problematiche](#problematiche)
      - [Distribuzione Normale Univariata](#distribuzione-normale-univariata)
      - [Distribuzione Normale Multivariata](#distribuzione-normale-multivariata)
      - [Pro e Contro](#pro-e-contro)
    + [Approccio Proximity Based](#approccio-proximity-based)
      - [Pro e Contro](#pro-e-contro-1)
    + [Approccio Density Based](#approccio-density-based)
      - [Pro e Contro](#pro-e-contro-2)
    + [Approccio Clustering Based](#approccio-clustering-based)
      - [Determinazione di quanto un oggetto appartiene ad un cluster](#determinazione-di-quanto-un-oggetto-appartiene-ad-un-cluster)
      - [Impact of Outlier on the Initial Cluster](#impact-of-outlier-on-the-initial-cluster)
      - [Il numero di cluster da utilizzare](#il-numero-di-cluster-da-utilizzare)
      - [Pro e Contro](#pro-e-contro-3)
    + [Approccio Reconstruction Based](#approccio-reconstruction-based)
      - [Pro e Contro](#pro-e-contro-4)
    + [Approccio One Class SVM](#approccio-one-class-svm)
      - [Pro e Contro](#pro-e-contro-5)
    + [Approccio Information Theoretic](#approccio-information-theoretic)
      - [Pro e Contro](#pro-e-contro-6)
  * [Valutazione dell'Anomaly Detection](#valutazione-dell-anomaly-detection)
- [Dimensionality Reduction](#dimensionality-reduction)
  * [Curse of Dimensionality ‚ò†Ô∏è](#curse-of-dimensionality---)
  * [Feature Selection](#feature-selection)
  * [Feature Extraction](#feature-extraction)
    + [Tecniche Lineari](#tecniche-lineari)
    + [PCA](#pca)
      - [Pro e Contro](#pro-e-contro-7)
    + [Crowding Problem](#crowding-problem)
    + [t-SNE](#t-sne)
      - [Pro e Contro](#pro-e-contro-8)
  * [Representation Learning (aka Features Learning)](#representation-learning--aka-features-learning-)
    + [Natural Language Processing üóª ü™µ](#natural-language-processing------)
      - [World Embedding](#world-embedding)
        * [Word2Vec](#word2vec)
- [Recurrent Neural Networks](#recurrent-neural-networks)
  * [Struttura Base](#struttura-base)
    + [Sequence To Sequence Learning](#sequence-to-sequence-learning)
  * [BPTT](#bptt)
  * [Deep Recurrent Networks](#deep-recurrent-networks)
  * [Il problema delle Long-Term Dependencies](#il-problema-delle-long-term-dependencies)
  * [Gestire le Long-Term Dependencies](#gestire-le-long-term-dependencies)
    + [Skip Connection](#skip-connection)
    + [Leaky Units](#leaky-units)
    + [Gated RNN :family_man_man_girl_boy:](#gated-rnn--family-man-man-girl-boy-)
    + [LSTM](#lstm)
    + [GRU](#gru)
    + [Gradient Clipping](#gradient-clipping)

## Definizione di un problema di learning

Un problema di learning ben posto ha 3 componenti:

- `T`: task
- `P`: performance measure
- `E`: Experience

Date queste 3 componenti, possiamo affermare che un programma del computer apprende se le sue performance per risolvere un task `T`, misurate da `P`, migliorano all'aumentare dell'esperienza `E`.

Una definizione pi√π informale √® la seguente: "_L'apprendimento √® un qualsiasi processo per cui un sistema migliora le proprie performance dall'esperienza._"

## Quando usare il Machine Learning

L'impiego del Machine Learning pu√≤ essere utile nei seguenti casi:

- Quando non esiste esperienza umana (navigare su marte)
- Quando gli uomini non possono spiegare la loro esperienza (speech recognition)
- Quando i modelli devono essere personalizzati (medicina personalizzata)
- Quando i modelli si basano su quantit√† di dati enormi (genomica)

Alcuni campi in cui il Machine Learning ottiene ottimi risultati sono:

- Riconoscimento dei pattern (scrittura a mano, linguaggio parlato)
- Generazione di pattern (generazioni di immagini)
- Riconoscimento delle anomalie (transazioni anomale di un conto bancario)
- Predizione (andamento di mercato)

## Tipi di Learning

* **Supervised Learning (inductive)**: vengono passati i dati per fare il training e i corrispettivi output
* **Unsupervised Learning**: vengono forniti solamente i dati per il training e non gli output
* **Semi-supervised Learning**: vengono forniti i dati per fare il training e parte degli output
* **Reinforcement learning**: ogni sequenza di azioni corrisponde ad una ricompensa positiva o negativa (rewards).

### Supervised Learning

Date le coppie di input e label $(x_1,y_1), (x_2,y_2), ...$, guardando il loro andamento √® possibile trovare una funzione $f(x)$ che permette di predire $y$ dato $x$.

In base al tipo di dato che √® $y$, possiamo avere 2 tipi di supervised learning differenti:

- `y` √® un **numero reale**: regressione ![regressione](./imgs/regressione.png)
- `y` √® **categorico**: classificazione ![classificazione](./imgs/classificazione.png)

**Unsupervised Learning**

L'utilizzo principale dell'Unsupervised Learning √® l'individuazione di pattern nascosti nei dati di input.
Delle applicazioni concrete possono essere:

* l'organizzazione di cluster di computer
* analisi di social network
* segmentazione del mercato
* astronomical data analysis

**Reinforcement learning**

Il Reinforcement Learning si basa su un sistema di reward ritardato con il quale viene fornita in output una policy che √® una mappatura stato - azione (in un dato stato ti dice quale azione eseguire).
Alcuni esempi possono essere:

* Game Playing
* Robot in a maze
* Bilanciare un palo nella mano

![reinforzo](./imgs/rinforzo.png)

## Progettare un sistema di Learning

Per progettare un sistema di leaning vanno seguiti i seguenti passi:

1. Scegliere il tipo di learning (supervised, unsupervised, ...)
2. Scegliere che cosa si vuole imparare (l'obbiettivo da raggiungere)
3. Scegliere come rappresentare l'obbiettivo
4. Scegliere un algoritmo di learning per dedurre la funzione obbiettivo dall'esperienza

![progettare_ai](./imgs/progettare_ai.png)

Ogni algoritmo √® costituito da 3 componenti:

* Rappresentazione
* Ottimizzazione
* Valutazione

Alcune funzioni di rappresentazione:

* Funzioni Numeriche
  * Regressione Lineare
  * Neural Network
* Funzioni simboliche
  * Decision Tree :palm_tree:
* Funzioni Instance-based
  * Nearest neighbor
* Modelli Probabilistic Graphical
  * Naive Bayes
  * Hidden Markov Models (HMMS)
  * Probabilistic Context Free Grammars (PCFGs)
  * Markov Networks

Algoritmi di Ricerca/Ottimizzazione:

* Gradient decent
  * Perceptron ([Pompotron](https://www.youtube.com/watch?v=0YQmE21aMnw&ab_channel=OrionNebula))
  * BackPropagation
* Dynamic Programming
  * HMM Learning
  * PCFG Learning
* Divide and Conquer
  * Decision tree induction
  * Rule learning
* Evolutionary Computation
  * Genetic Algorithm (GAs)
  * Genetic Programming (GP)
  * Neuro evolution

Alcuni criteri di valutazione:

* Accuracy
* Precision ad Recall
* Squared error
* Likelihood
* Posterior probability
* Cost / Utility
* Margin
* Entropy
* K-L Divergence
* ecc.

## Classificazione

**Definizione Classificazione**: dato un training set, ogni elemento √® caratterizzato da una tupla $(x, y)$ dove:

* $x$ √® un insieme di attributi (_input_)
* $y$ √® il nome della classe (_label_)

Il nostro obbiettivo √® quello di imparare un modello che mappa ogni set di attributi $x$ in una data classe $y$.

![schema_classificazione](./imgs/schema_classificazione.png)

Ci sono vari modi per effettuare la classificacazione che si dividono in 2 principali categorie:

* Base Classification
  * Decision Tree :palm_tree:
  * Rule Based
  * Nearest Neighbor
  * Neural Networks
  * Deep Learning
  * Naive Bayes and Bayesian Belief methods
  * Support Vector Machines
* Ensemble Classification
  * Boosting
  * Bagging
  * Random Forest (:palm_tree: :palm_tree: ? :palm_tree: ?? :palm_tree:)

### Decision Treeee :evergreen_tree:

![decision_tree_example](./imgs/decision_tree_example.png)

ALcuni algoritmi per la classificacazione basati su decision tree sono:

* Hunt's Algorithm
* CART(s)
* ID3 (volkswagen), C4.5 (Picasso Citroen)
* SLIQ, SPRINT

### Hunt's Algorithm

**Funzionamento**: sia $D_t$ un set di dati di training si ha la seguente procedura:

* se $D_t$ contiene record che appartengono alla stessa classe $y_t$, allora $t$ √® un nodo foglia ed appartiene alla classe $y_t$
* se $D_t$ contiene record che appartengono a pi√π di una classe, allora testa un attributo per dividere i dati in sottoinsiemi pi√π piccoli. Poi viene applicata ricorsivamente la procedura di prima.

![hunt_example](./imgs/hunt_example.png)

## Problemi di design di alberi ad induzione elettromagnetica :zap:

Due problemi principali sono:

* la determinazione di una metodologia di split che dipende da due fattori principali:
  * specifica di una condizione di test, che dipende dal tipo di attributo
  * unit√† di misura per valutare la correttezza di una misura di test (quanto bene un attributo rappresenta la classe)
* determinare quando terminare la divisione:
  * fermarla anticipatamente (Early termination :robot:)
  * fermarla se tutti i record appartengono alla stessa classe (foglia raggiunta) oppure se hanno tutti gli attributi uguali (porteranno allo stesso risultato)

### Metodi per effettuare i test

Questi metodi variano in base al:

* Tipo di Attributo
  * Binary
  * Nominal
  * Ordinal
  * Continussssss
* Numero di vie per lo split
  * 2 way split
  * multy way split

**Multiway or Binary Split**:

![multi_binary](./imgs/multi_binary.png)

**Splitting di Attributi Continui**

Possono essere gestiti in 2 modi principali:

* Discretizzazione: permettono di formare categorie ordinali. Possono essere raggruppati in cluster, in insiemi di frequenze equivalenti o intervalli equivalenti. Questa divisione pu√≤ essere effettuata in maniera **Statica** (solo all'inizio) o **Dinamica** (per ogni nodo)
* Decisione Binaria: esegue dei test binari come $A > v$ o $A <= V$

Un possibile approccio √® quello greedy che guarda l'indice di purezza della divisione delle classi:

![purezza](./imgs/purezza.png)

Indici per misurare l'impurit√† sono :

* GEEEEENEEEEEE Index<br> ![gini](./imgs/gini.png)
* Entropy<br> ![entropy](./imgs/entropy.png)
* Misclassification error<br> ![error](./imgs/error.png)

**Come scegliere lo split migliore**:

1. Calcolare l'indice di impurit√† `P` prima dello split
2. Calcolare l'indice di impurit√† `M` dopo lo split
   * `M` √® l'impurit√† pesata dei figli
3. Scegliere l'attributo che produce il valore `Gain` maggiore: `Gain = P - M` (equivalente a scegliere l'attributo con `M` minore)

![grafo](./imgs/grafico.png)

#### Gini

`p(j|t)` √® la frequenza relativa della classe j al nodo t

Il valore massimo che pu√≤ assumere √® $1 - \frac{1}{nc}$ quando i record sono distribuiti in maniera equa e quindi si ha un alto livello di impurit√†

Il minimo √® 0.0 che implica che titti i record appartengono ad una sola classe (l'informazione pi√π interessante)

L'indice Gini √® usato negli algoritmi CART, SLIQ SPRINT

#### Entropy

L'entropia √® come l'indice geany, serve per trovare lo split migliore (quello che ha valore di entropia pi√π vicino allo 0). Questi indici per√≤ tendono a tenere in considerazione la purezza degli attributi, senza tenere conto della larghezza dell'albero.

Per aggirare questo problema viene introdotto il Gain Ratio che penalizza le partizioni piccole con molti elementi.

![gain_ratio](./imgs/gain_ratio.png)

#### Pro Vs Cons

*Pro*:

* Costruzione poco costosa
* Incredibile velocit√† della classificazione di record sconosciuti :rocket:
* Di facile interpretazione per alberi di piccole dimensioni
* Resistente al rumore (IP68)
* Pu√≤ gestire facilmente attributi ridondanti o irrilevanti

*Contro*:

* lo spazio delle decisioni pi√≤ essere esponenziale e quindi l'approccio greedeeeee non riesce spesso a trovare l'albero migliore
* Non considera le interazioni tra gli attributi
* Il confine decisionale considera solo un attributo alla volta

### Caratteristiche degli Alberi

Se viene utilizzato 1 attributo per test condition le decision boundary corrisponderanno a rette perpendicolari agli assi dei corrispondenti attributi.

![rette](./imgs/rette.png)
_Test condition con un singolo attributo_

Per avere delle decision boundary pi√π elaborate √® necessario creare test condition che considerano attributi multiply.

![attributimultipli](./imgs/attributimultipli.png)
_Test condition con pi√π attributi: `x + y < 1`_

## Errori

Ci sono 3 tipi di errori:

* **Training**: sono gli errori effettuati durante la fase di training (utilizzando dataset di training)
* **Testing**: sono gli errori effettuati durante la fase di testing (utilizzando il dataset di testing)
* **Generalization**: l'errore intrinseco del modello testato su un insieme di record non precedentemente visti appartenenti alla stessa distribuzione

### Overfitting e Underfitting

![overfitting](./imgs/overfitting.png)

**Overfitting**:
Se i dati di training sono sotto rappresentativi (non rappresentano bene l'ambiente), all'aumentare dei nodi aumentano gli errori di testing e diminuiscono gli errori di training. Aumentando la dimensione dei dati di training riduce questa differenza tra i dati ad un qualsiasi numero di nodi.

__In breve__: se vengono forniti dati che non rappresentano completamente il problema allora l'algoritmo andr√† ad imparare solamente come risolvere quelle situazioni e non riuscir√† a gestirne di diverse (esempio: vengono fornite 2 razze di primati per il problema del riconoscimento di scimmie, l'algoritmo imparer√† a conoscere perfettamente quelle 2 razze, ma quando gli verr√† presentata una nuova razza non comprender√† che √® una scimmia).

Le cause dell'overfitting sono:

* Dimensioni dei dati di Training limitate
* Alto livello di complessit√† del modello

Decision Trees molto pi√π complessi del dovuto portano all'Overfitting.
Gli errori di Training non forniscono una buona stima degli errori di Testing.

**Underfitting**:
Quando un modello risulta troppo semplice e non √® in grado di classificare correttamente i dati.

## Model Selection

Serve per valutare la bont√† di un dato modello, e quindi per evitare che incappi in overfitting stimandone il Generalization. Il **Generalization Error** pu√≤ essere calcolato nei seguenti modi:

* Usando un **Validation Set**
  * E' un set di dati, diverso dal training, che serve per stimare quanto sia affidabile il modello, ma non √® sufficiente per il testing (esempio dell'esame di Bartoli). Si creano e allenano pi√π modelli differenti e con il validation set si sceglie quello pi√π preciso.
* Incorporando la **Model Complexity**
  * Un'alta complessit√† tende a causare un numero maggiori di errori, quindi, dati 2 modelli √® sempre meglio preferire quello con complessit√† minore. La complessit√† equivale a: $GenError(Model) = TrainError(Model, TrainData) + a \cdot Complexity(Model)$
* Stimando i Limiti Statistici

### Approccio pessimistico

Questa formula serve per calcolare il Generalization Error e quindi la complessit√† del Decision Tree

![pessimo](./imgs/pessimistico.png)

_E' equivalente alla formula_ $GenError(Model) = TrainError(Model, TrainData) + a \cdot Complexity(Model)$

### Approccio Ottimistico

Nel caso in cui non si voglia calcolare il Generalization Error, pu√≤ essere fatta una stima molto ottimistica dell'errore con il Training Error.

### PrePruning

Per evitare che un modello incappi in overfitting si pu√≤ applicare la strategia del pruning: ovvero la potatura di alcune foglie per semplificare l'albero.

Il PrePruning avviene prima del completamento del Decision Tree e per decidere quando potare vengono usate dei valori di threshold che, se superati, portano all'eliminazione di un sotto albero.

![preprunin1](./imgs/prepruning1.png)
![prepruning2](./imgs/prepruning2.png)

### PostPruning

E' simile al prepruning solo che la potatura viene effettuata solo dopo che il Decision Tree viene calcolato completamente, con modalit√† Bottom-Up.
La classe scelta per sostituire il sotto albero sar√† quella con il numero maggiore di record.
E' pi√π preciso del PrePruning per√≤ richiede pi√π calcoli.

## Valutazione delle Performance di un Classificacatore

Ci sono vari modi per valutare le performance di un classificacatore:

* **Metodo Holdout**: consiste nel dividere i dati originali in 2 set: uno di training e uno di testing (la divisione √® a discrezione dell'analista). Successivamente il classificatore viene allenato col set di training e poi viene testata la sua accuratezza con il set di testing. Questo modello presenta svariati problemi: se forniamo troppi dati di testing e pochi di training, il modello potrebbe non operare al massimo delle sue potenzialit√†, mentre se vengono forniti troppi dati di training e pochi di testing, la stima finale potrebbe non essere affidabile al 100%. Infine, poich√© i set di training e di testing sono derivati dallo stesso insieme di dati, potrebbe capitare che uno dei 2 sottoinsieme sia pi√π rappresentativo del dataset originale, mentre l'altro no. Per migliorare la precisione di questo metodo pi√≤ essere applicato il Random Subsampling che consiste nel ripetere pi√π volte l'allenamento e il testing con sotto set differenti per ogni iterazione.
* **Cross-Validation**: un'alternativa al Random Subsampling √® il Cross-Validation che consiste nel dividere il dataset in `k` partizioni di dimensioni equivalenti e successivamente di utilizzare `k-1 ` partizioni per il training e 1 per il testing. Queste partizioni si scambieranno fin quando tutti gli elementi verranno utilizzati per il testing 1 sola volta. Un metodo speciale √® il _leave on out_, che √® simile al metodo descritto sopra ma ha `k = N` (dove `N` √® la dimensione del dataset) e consiste nell'usare un solo record alla volta per il testing. Questa procedura risulta molto precisa ma molto costosa.

## Imbalanced Class Problem ‚öñÔ∏è

In alcuni problemi di classificazione il numero di record delle classi potrebbero essere sbilanciati. Come si pu√≤ valutare la precisione di un classificatore con queste caratteristiche ??

### Precision & Recall

Per valutare la bont√† di un classificatore possiamo utilizzare varie metriche, la pi√π comune √® l'Accuracy. Tuttavia, se √® pi√π importante che il modello rilevi la "classe rara" (e.g. transazione fraudolenta su dataset di transazioni) l'accuracy non √® molto utile perch√© il suo valore viene incrementato notevolmente dai record dell'altra classe (quella pi√π comune).

Per visualizzare meglio il problema possiamo utilizzare la **confusion matrix**, una matrice che mette in evidenza come vengono classificati i vari record.

![confusione](./imgs/confusione.png)

Basandoci sulla confusion matrix possiamo calcolare l'accuracy come: ![accuracy](./imgs/accuracy.png)

Esistono altre metriche importanti per valutare meglio i classificatori:

- **Precision**: serve a mettere in evidenza i _false positive_, pi√π √® alta la percentuale meno ce ne sono. (verticale) ![precision](./imgs/precision.png)
- **Recall**: serve a mettere in evidenza i _false negative_, pi√π √® alta la percentuale meno ce ne sono. (orizzontale) ![recall](./imgs/recall.png)
- **F-measure**: sintetizza in un'unica misura i valori di Precisione e Recall. ![fmeasure](./imgs/fmeasure.png)

Ne esistono tantissime altre che vanno utilizzate in base al problema specifico che andiamo ad affrontare (generalmente Precision e Recall sono quelle pi√π utili ed efficaci).

![misure](./imgs/misure.png)

**Esempio**:

Nel seguente esempio possiamo vedere come l'Accuracy √® alta ma i valori di Precision e Recall sono estremamente bassi, indice di un classificatore dalle scarse performance (tabella 1).

![esempio](./imgs/precision_esempi.png)

### ROC ü™®

<img src="./imgs/wock.png" align=right width="300px">

Un metodo grafico per confrontare vari modelli √® quello di utilizzare le curve **ROC** (Receiver Operating Characteristics). Esse rappresentano la relazione tra True Positive Rate (**TPR**, asse y) e False Positive Rate (**FPR**, asse x) mostrando i compromessi tra TPR e FPR.

Nelle ROC ci sono dei punti che hanno un'interpretazione ben conosciuta:

- (0, 0): un modello che predice tutte le istanze come negative
- (1, 1): un modello che predice tutte le istanze come positive
- (1, 0): modello ideale con assenza di False Positive (le azzecca tutte)

La retta che congiunge i punti (0, 0) e (1, 1) rappresenta il Random Guessing. Se un grafico si avvicina a questa il modello fa molto schifo.

Se un grafico si trova pi√π a sinistra dell'altro (pi√π verso (1, 0)) allora √® migliore localmente. Non possiamo per√≤ affermare che uno √® meglio dell'altro in generale.

![roc](./imgs/roc.png)

Nella figura precedente possiamo vedere che il modello M1 risulta migliore con un FPR inferiore di ~0.36, invece M2 √® migliore con un FPR maggiore. Non possiamo per√≤ affermare che uno sia meglio dell'altro globalmente.

Un buon indice per valutare quando un modello √® migliore di un altro in generale √® calcolare l'area sotto la curva ROC, se questa √® maggiore di un altro modello possiamo affermare che questo √® migliore dell'altro globalmente. Se l'area di una ROC √® 0.5 rappresenta un modello equivalente al Random Guessing. Un modello ideale ha l'area uguale ad 1.

#### Generazione di una ROC

Le curve ROC possono essere rappresentate solo se il modello produce output continui che verranno utilizzati per fare il ranking delle predizioni. Questi output continui potrebbero essere le Posterior Probabilities generate da un Naive Bayesian Classifier o valori numerici prodotti da un ANN.

Assumendo che gli output del modello siano continui per tracciare la curva ROC bisogna:

1. Ordinare tutti i record di test in ordine crescente in base al loro valore di output
2. Selezionare il record con rank pi√π basso e assegnare questo e tutti gli altri record sopra di lui come Positive Class (equivale a classificare tutto come Positive Class, TPR = FPR = 1)
3. Selezionare il record successivo, classificare il record selezionato e tutti quelli superiori come Positive e quelli sotto di esso come Negative. Aggiornare il TPR e FPR come segue:
   - se il record precedente √® Positive, decrementa il TP Count e lasciare inalterato i FP Count
   - se il record precedente √® Negative, decrementa i FP Count e lascia inalterati i TP Count
4. Ripeti lo step 3 fin quando non raggiungi il record con il rank pi√π alto.
5. Stampa TPR e FPR

![rooc_do](./imgs/roc_do.png)

### Conclusione del problema

Per risolvere il problema delle classi sbilanciate bisogna:

- Undersample della classe pi√π popolosa
- Oversample della classe pi√π rara.

## Nearest Neighbor Classification

Esistono dei tipi di algoritmi di learning che non costruiscono un modello a priori per classificare i dati ma che li classificano solamente nel momento del bisogno, essi sono detti lazy learners. Un esempio √® il Rote Classifier che si ricorda tutti i suoi esempi di training e una volta passati dei dati di testing li classifica solo se corrispondono esattamente a dati gi√† visti nella fase di training. Questo implica una scarsa flessibilit√† nella classificazione. E' stato quindi ideato un approccio pi√π generale chiamato Nearest Neighbor Classifier.

Esso si basa sullo stesso concetto della Rote Classifier ma non guarda l'equivalenza ma la similarit√† tra il dato di testing e quelli di training, ovvero cerca i vicini pi√π vicini al record di testing.

I dati con `n` attributi vengono rappresentati su uno spazio n-dimensionale e la precisione della classificazione dipende da una variabile distanza `k`. Ci sono altre varianti di questo algoritmo che alterano il modo di determinare i vicini pi√π vicini basandosi non solo sulla distanza ma anche sulla _classe di maggioranza_, oppure sulla _classe di maggioranza con distanze pesate_ dove non conta solamente la classe che compare pi√π volte ma anche la sua distanza dal record di testing (pi√π lontano sar√† e minore sar√† l'importanza).
√à importante notare che un numero `k` troppo alto di vicini potrebbe includere anche classi errate ed un numero troppo piccolo risulta essere molto sensibile al rumore.

![near](./imgs/near.png)
![voronoioioioioi](./imgs/voronoi.png)
![euclide](./imgs/euclide.png)

_Diagramma di Voronoi per 1-nearest Neighbor e Distanza Euclidea_

### Vantaggi

* Non hanno bisogno di mantenere un modello astratto derivato dai dati
* Non richiedono model building
* Poich√© possono generare decision boundaries arbitrariamente dispongono di una maggiore flessibilit√† rispetto agli eager learner

### Svantaggi

- Richiedono pi√π computazione degli eager learner nella fase di testing
- Poich√© fanno classificazioni basate su informazioni locali sono molto suscettibili al rumore
- Possono generare errori di classificazione se non avvengono step di preprocessing (aggiustamento delle scale dei dati)

## Bayesian Classification

Un classificatore bayesiano basa il suo processo di learning su un importatane teorema statistico: Il teorema di Bayes.

![bayes](./imgs/bayes.png)

Questo teorema fornisce un modo per revisionare delle predizioni o teorie esistenti, aggiornandone le probabilit√† in seguito alla scoperta di informazioni aggiuntive.
Il teorema afferma che: la _probabilit√† a posteriori_ `P(Y|X)` √® data dal prodotto della _probabilit√† condizionale di classe_ `P(X|Y)` per la _probabilit√† a priori_ `P(Y)` fratto le _nuove informazioni_ `P(X)`

**N.B.** quando si confrontano varie probabilit√† per differenti valori di Y, il denominatore pu√≤ essere ignorato.

Tale teorema pu√≤ essere applicato da un algoritmo di ML in due modi in base a come viene implementato il calcolo della _probabilit√† condizionale di classe_:

* Naive
* Belief Network

Per la classificazione si va a vedere il valore pi√π alto tra le varie probabilit√† a posteriori `P(Y|X)` e la classe con probabilit√† maggiore sar√† la vincente.

### Naive Bayesian Classifier

Il metodo Naive calcola il valore di `P(X|Y)` nel seguente modo:
![produttoria](./imgs/produttoria.png)

Va per√≤ fatta una distinzione in base ai tipi di attributo che si prendono in considerazione:

* **Categorici**: si calcola il rapporto tra il numero di volte che l'attributo compare all'interno dei record che contengono la classe in questione fratto il numero di volte che compare la classe Y in questione

* **Continui**: per trattare questi dati si pu√≤ procedere in 2 modi diversi:
  
  * **Discretizzando**: si dividono i dati in intervalli pi√π piccoli trasformando quindi l'attributo continuo in un attributo categorico e si procede come visto sopra. Bisogna fare attenzione a come vengono scelti gli intervalli: troppo grandi sono poco precisi e troppo piccoli causano overfitting
  * **Utilizzano le distribuzioni di Probabilit√†**: si cerca una distribuzione di probabilit√† pi√π adatta alle variabili continue e si stimano i parametri della distribuzione usando i dati di training. Generalmente la distribuzione Gaussiana √® la pi√π utilizzate e quindi ne deriva la seguente formula: ![gauss](./imgs/gauss.png)

Se una probabilit√† condizionale √® `0` allora verr√† azzerata tutta l'espressione. Per questo motivo sono state implementate delle variazioni che permettono di evitare il problema:

![variazioni](./imgs/variazioni.png)

#### In Breve

* Sono resistenti a punti di rumore isolati che vengono cancellati durante i calcoli
* Sono resistenti ad attributi irrilevanti
* Le performance vengono peggiorate da attributi correlati perch√© non esiste pi√π l'assunzione dell'indipendenza condizionale (per risolvere questo problema si usa il BBN spiegato dopo)

### Bayesian Belief Network

Se sono presenti degli attributi correlati questo algoritmo offre performance migliori. Esso fornisce una rappresentazione grafica delle relazioni probabilistiche tra un insieme di variabili random tramite un DAG (Grafo Orientato Aciclico).
A seconda del numero di nodi padri viene fatta una distinzione:

* Se non ha genitori allora contiene la probabilit√† a priori `P(X)`
* Se ha 1 solo genitore, contiene la probabilit√† condizionale `P(X|Y)`
* Se ha pi√π genitori, contiene la probabilit√† condizione `P(X|Y1, Y2, ..., Yn)`
  ![dag](./imgs/dag.png)

Queste probabilit√† vengono poi inserite in una tabella relativa ad ogni nodo. Durante la classificazione vengono presi questi valori per calcolare la classe di appartenenza.

#### In Breve

* Permette la visualizzazione grafica tramite un DAG
* La prima costruzione richiede molto tempo e risorse, ma una volta costruito √® di facile gestione
* Gestiscono facilmente i dati incompleti (attributi mancanti)
* Resistente al Model Overfitting

## Support Vector Machine üé∞

Le Support Vector Machine sono una tecnica di classificazione basata sullo _statistical learning_ che di recente ha visto un incremento di interesse nell'ambito della ricerca. Si presta bene alla classificazione e riconoscimento di testi.

La sua applicabilit√† ricade principalmente in casi in cui i dati sono linearmente separabili, ma sono state studiate strategie per poterle applicare anche negli altri casi.

![svm](./imgs/svm.png)

Il principio di funzionamento di questi classificatori si basa nel suddividere l'insieme dei dati di training con un iperpiano affinch√© i dati a destra e a sinistra dell'iperpiano facciano parte di classi distinte (linearmente separabili).

_In breve_: individuazione dell'iperpiano che separa i dati.

Tuttavia, per un dato set di dati √® possibile trovare infiniti iperpiani che separano i dati, dunque √® importante trovare l'iperpiano che dia i risultati migliori. Per farlo √® necessario introdurre il concetto di **margine**: il margine pu√≤ essere identificato come la distanza tra le rette parallele all'iperpiano passanti per i vettori di supporto (i punti del dataset) di classi diverse pi√π vicini.

![support vector](./imgs/support_vector.png)

Per ottimizzare il modello, l'algoritmo dovr√† cercare i vettori di supporto pi√π vicini andando a massimizzare il loro margine, poich√© un margine ampio genera un minore errore di classificazione (su record non visti precedentemente) e riduce l'overfitting.

### Classificazione

Essendo l'iperpiano una retta, il decision boundary del modello pu√≤ essere rappresentato con la seguente formula:

![db](./imgs/svmdb.png)

dunque, per poter classificare i dati situati al disopra e la disotto del decision boundary, per un dato input (`z`) dovremmo risolvere la seguente disequazione:

![dbdis](./imgs/dbdis.png)

### Training - Caso Separabile

Come per tutti i classificatori lineari, l'obiettivo di training sar√† quello di stimare i parametri `w` e `b` per determinare un decision boundary, questi parametri andranno scelti in modo tale da rispettare le seguenti condizioni:

![limiti](./imgs/limiti.png)

Tuttavia per le SVM √® necessario un requisito aggiuntivo, ovvero quello di massimizzare il margine relativo al decision boundary. Il margine pu√≤ essere ricavato tramite la seguente formula:

![distanza](./imgs/dis.png)

Dobbiamo quindi massimizzare questa distanza, ma per motivi matematici e di semplificazione dei calcoli possiamo riscriverla nel seguente modo e minimizzarla (risulter√† pi√π semplice minimizzare che massimizzare):

![minimizzazione](./imgs/mini.png)

Riassumendo, il processo di training di una Linear SVM con dati separabili, pu√≤ essere formalizzato con il seguente problema di ottimizzazione vincolato:

![lsvm](./imgs/lsvm.png)

Per risolvere effettivamente questo problema, sar√† necessario riscrivere la funzione obbiettivo come Lagrangiana affinch√© essa tenga conto dei vincoli imposti alle sue soluzioni.

![lagrange](./imgs/lagrange.png)

_Ulteriori spiegazioni sono delegate al Dott. Cristian Cosci :scroll:._

### Training - Caso non Separabile

![soft margin](./imgs/soft.png)

Come si vede dall'immagine, `B2` √® l'unico margine privo di errori, tuttavia essendo piccolo √® molto suscettibile all'overfitting. Per questo motivo pu√≤ essere una buona opzione scegliere il margine `B1` che, anche se presenta dei piccoli errori, √® molto probabile che dia performance che siano migliori in generale. Questo approccio √® detto **Soft Margin** e consiste nel trovare un compromesso tra larghezza del margine e numero di errori commessi nella fase di training. Questo permette anche di risolvere semplici problemi non linearmente separabili.

La funzione per la massimizzazione del margine rimane invariata rispetto a quella dell'approccio per dati separabili, tuttavia bisogner√† cambiare le restrizioni che verranno invalidate dal nostro nuovo approccio. Sar√† necessario avere una piccola soglia di tolleranza agli errori durante la fase di training, questo viene raggiunto introducendo le *Slack Variables* che forniscono una stima dell'errore per il decision boundary su un dato esempio di training.

![slack](./imgs/slack.png)

Tuttavia, applicando questa definizione direttamente all'algoritmo di training, non viene imposto alcun vincolo sul numero di errori che possono essere commessi e di conseguenza, l'algoritmo troverebbe un margine molto ampio ma pieno di errori in fase di training. Per evitare il problema √® necessario penalizzare decision boundary con un valore delle slack variable alto.

![slack_pena](./imgs/slack_pena.png)

I parametri `C`e `k` rappresentano quanto l'errore penalizzi il modello. Per esempio:

- con valori di C **grandi**, il peso delle violazioni aumenta e di conseguenza si avr√† un margine piccolo che porter√† ad overfitting
- con valori di C **piccoli**, si avr√† un margine ampio con molti errori ed un elevato bias

### Training - Caso Non Lineare

Gli approcci definiti fino ad ora non sono applicabili agli spazi di training non linearmente separabili, dunque √® necessario trovare un nuovo approccio che consiste nel trasformare lo spazio di partenza $x$ in uno spazio linearmente separabile $\Phi(x)$.

![fi](./imgs/fi.png)

Come possiamo vedere da questo esempio, il decision boundary che originariamente era circolare viene linearizzato applicando la trasformazione non lineare $\Phi(x)$.

![fi no](./imgs/fi_no.png)

Il nuovo learning task potr√† essere formalizzato come il seguente problema di ottimizzazione:

![](./imgs/fi_nuovo.png)

L'unica differenza con il caso linearmente separabile √® che il vincolo viene calcolato non pi√π sul set degli attributi base $x$ ma, sulla loro trasformazione $\Phi(x)$, che √® un vettore con tante componenti quante sono le dimensioni dello spazio trasformato.

Sviluppando la Lagrangiana, il calcolo si ridurr√† a un *dot product* (prodotto scalare o similarit√†) tra una coppia di vettori dello spazio trasformato ($\Phi(x_i) \cdot \Phi(x_j)$ ). Tuttavia questo calcolo risulta essere tremendamente costoso e per causa sua possiamo incappare nella **Maledizione della Dimensionalit√†**: aumentando il numero di dimensioni (features) il quantitativo di dati necessario per generalizzare con precisione aumenta esponenzialmente (servono tantissimi dati di training per permettere al modello di apprendere tutte le possibili combinazioni di feature possibili)!

Questo problema pu√≤ essere aggirato tramite il **Kernel Trick**.

Il kernel trick √® un metodo che permette di calcolare la similarit√† tra 2 vettori partendo dal set di attributi originale, che pu√≤ essere espresso come segue:

![kernel trik](./imgs/kernel_trik.png)

La sua applicabilit√† richiede che sia vera la seguente affermazione: deve esistere una trasformazione corrispondente tale che la kernel function calcolata per una coppia di vettori √® equivalente al dot product dei vettori nello spazio trasformato. Tale propriet√† viene espressa dal teorema di Mercer.

![mercer](./imgs/mercer.png)

In sostanza, applicando la funzione kernel si evita di calcolare le $\Phi$, riducendo il costo computazionale dell'algoritmo.

### Caratteristiche

Le caratteristiche generali delle SVM sono le seguenti:

- Il learning problem pu√≤ essere formulato come un problema di ottimizzazione convesso per i quali sono disponibili algoritmi molto efficienti che garantiscono il ritrovamento di un minimo globale (che fornisce le performance migliori)
- Performa il Capacity Control, massimizzando il margine del Decision Boundary
- Pu√≤ essere applicato a variabili categoriche introducendo Dummy Variables per ogni categoria degli attributi
- Sono applicabili a problemi multi classe

## Ensemble Methods

Gli ensemble methods sono delle tecniche di classificazione che basano il loro principio di funzionamento sul combinare vari classificatori per ottenere un risultato pi√π preciso.
√à stato osservato che i classificatori di base non devono essere correlati tra di loro perch√© in quel caso l'errore di generalizzazione totale non verr√† migliorato dal metodo ensemble (meglio classificatori indipendenti).

![ensamble](./imgs/ensable.png)

Dall'immagine possiamo notare come i metodi ensemble vanno a migliorare il generalization error totale solo fino a quando il generalization error dei modelli di base √® migliore del random guessing (<.5). La linea tratteggiata mostra le performance di classificatori base correlati tra di loro, mentre la riga continua mostra le performance di classificatori base indipendenti.

### Metodi per costruire Ensemble Classifier

1. **Manipolando i dati di training (training set)**: vengono creati pi√π set di training basandosi su una qualche distribuzione di campionamento. Dopodich√© viene creato un classificatore utilizzando degli algoritmi particolari come _Bagging_ e _Boosting_.
2. **Manipolando le input features**: da ogni dataset viene scelto un sottoinsieme di input features che verranno poi utilizzate per allenare i vari classificatori dell'ensemble.
3. **Manipolando le label di classe**: questo metodo si usa quando ci sono un grande numero di classi, il dataset di training viene trasformato in un problema di classificazione binario partizionando le label in due sottoinsiemi disgiunti $A_0$ e $A_1$ che verranno utilizzate successivamente per allenare un classificatore. Ripetendo questo step pi√π volte (relabeling) si otterr√† un ensemble di classificatori base. Quando poi verr√† passato un dato di test $x$ viene fatta la somma delle volte che viene classificato nella classe $A_0$ o nella classe $A_1$ e poi viene scelta la classe di maggioranza.
4. **Manipolando l'algoritmo di learning**: l'algoritmo di learning scelto viene applicato pi√π volte per ottenere pi√π modelli di base che poi verranno utilizzati per creare l'ensemble. Un esempio di modelli che si prestano meglio a questo processo sono i decision tree e le ann, perch√© variando i parametri (pesi, bias, topologia del modello, ecc.) con cui sono costruiti creano classificatori abbastanza diversi.

I primi 3 approcci sono metodi generici, mentre l'ultimo √® dipendente dal tipo di classificatore utilizzato.

Gli ensemble methods funzionano meglio con classificatori instabili, ovvero modelli molto sensibili a piccole perturbazioni nel dataset di training.

![ensamble algo](./imgs/ensamble_alg.png)
![ensamble code](./imgs/ensamble_code.png)

### Bias-Variance Decomposition

L'inabilit√† di un modello di machine learning nel catturare la vera relazione tra i dati √® chiamata **Bias** (min sqrt error, la distanza tra i pallini e la riga rossa).

![bias bello](./imgs/bias_bello.png)

A sinistra un esempio di bias elevato (c'√® molta distanza tra i pallini e la retta), a destra un esempio di bias nullo, la riga rossa riesce perfettamente a dividere ogni pallino.

La **Varianza** √® quanta differenza c'√® tra accuracy nel dataset di training ed in quello di testing (quanto classifica bene il training e il testing).

![varianza bello](./imgs/varianza_bello.png)

A sinistra il training set viene classificato perfettamente, a destra il testing set viene classificato abbastanza male (distanza tra pallini e linea) quindi abbiamo una varianza elevata.

Con un bias estremamente piccolo ed un elevata varianza siamo di fronte all'overfitting.

### Bagging

√à un metodo di classificazione che manipola i dati di training campionando con ripetizione l'insieme di training e ottenendo cos√¨ `n` sotto set, tutti della stessa dimensione del dataset originale,(fase di bootstrap) che verranno utilizzati per allenare `n` modelli dell'ensemble. I record vengono scelti secondo la distribuzione uniforme e dunque ogni campione di bootstrap conterr√† circa `63%` dei dati del set originale. Pu√≤ capitare che in alcuni campioni di bootstrap compaiano molteplici volte lo stesso record o che alcuni siano del tutto assenti.

Una volta allenati i modelli, per classificare un record mai visto prima viene effettuata una votazione di maggioranza e la classe con pi√π voti risulter√† la classe di output (aggregation).

![bagging](./imgs/bagging.png)
_Romani Artista üñåÔ∏è_

Il Bagging migliora l'errore di generalizzazione riducendo la varianza dei classificatori di base, questo perch√© le prestazioni del bagging dipendono dal classificatore di base:

- se il classificatore di base √® instabile, aiuta a ridurre gli errori associati alle fluttuazioni nei dati di training
- se il classificatore di base √® stabile (robusto a piccole perturbazioni nell'insieme di addestramento), l'errore dell'ensemble √® principalmente causato da bias nel classificatore di base. In questo caso il bagging potrebbe non migliorare le performance ma andare addirittura a peggiorarle.

### Boosting

Il Boosting √® una procedura iterativa che cambia in maniera adattiva la distribuzione dei campioni di training in maniera tale che vengano favoriti i campioni classificati erroneamente. Ad ogni campione viene assegnato un peso (nella fase iniziale hanno tutti lo stesso `1/N`, con `N` il numero di record) e si effettua un campionamento con ripetizione con cui verr√† costruito il primo modello. Successivamente il modello viene testato ed in base agli errori di classificazione commessi, i pesi dei campioni del dataset originale verranno aggiornati e si ripeter√† il processo fino ad ottenere il numero di modelli richiesto. I pesi dei record correttamente classificati verranno decrementati metre quelli misclassificati saranno aumentati per far si che nello step successivo verranno scelti.

![boosting](./imgs/boosting.png)
_Esempio di classificatori generati con 10 Round di Boosting_

Esistono differenti versioni che variano nel come i pesi vengono aggiornati e in come le predizioni fatte dai vari classificatori sono combinate. Una possibile implementazione di questo metodo √® l' **Ada Boosting**.

#### AdaBoosting

L'algoritmo di Ada Boosting funziona nel seguente modo:

1. Inizialmente viene assegnato ad ogni record del dataset lo stesso peso: `1/N`.
2. Per ogni feature del dataset viene generato un modello base (e.g. decision stump) e viene calcolato l'errore pesato `ei` di ogni modello. Se `ei` supera `0.5` allora i pesi vengono resettati ai valori di partenza (`1/N`). ![ada2](./imgs/ada2.png).
   Da questo errore √® possibile ricavarsi il parametro `aj` che verr√† utilizzato per aggiornare i pesi nello step successivo (√® tipo un indice di performance). ![perfomance](./imgs/adaperformance.png)
3. Aggiorna i pesi di ogni record basandosi sulla seguente formula: ![ada 1](./imgs/ada1.png). Se il record √® classificato correttamente il peso viene diminuito, altrimenti viene aumentato. `Zj` √® un fattore di normalizzazione che permette di far tornare la somma di tutti i nuovi pesi del dataset a 1 (ricondotto alla probabilit√† che venga scelto nella nuova istanza di training)(calcola i nuovi pesi e li normalizza).
4. Continua partendo dai nuovi pesi fin quando non si ottiene il numero di classificatori voluti.

Dato un test record, il risultato viene scelto basandosi su una media pesata dei risultati di classificazione di tutti i classificatori base. Si tende a dare pi√π peso ai classificatori con accuracy pi√π alta sfavorendo quelli dalle performance peggiori (che solitamente sono quelli generati durante le prime fasi di boosting).

√à importante notare che possiamo stimare l'errore di training dell'ensemble ed √® dato dalla seguente formula (pone un limite superiore):

![errore limite](./imgs/ada_errore.png)

Dato che questo algoritmo tende a concentrarsi su esempi di allenamento che vengono classificati in modo errato, la tecnica di boosting pu√≤ essere piuttosto suscettibile all'overfitting.

### Random Forest

L'algoritmo Random Forest combina le decisioni di pi√π alberi decisionali (ensamble). Ogni albero viene generato basandosi su valori di un vettore scelto a caso tramite una distribuzione di probabilit√† fissa (a differenza di quella dell'AdaBoosting che variava nel tempo).

Usare il Bagging con alberi decisionali √® un particolare tipo di Random Forest che serve per aggiungere casualit√† durante la costruzione del modello per evitare alberi troppo correlati tra di loro.

Si pu√≤ stimare un limite superiore del generalization error (a patto che il numero di alberi della random forest sia abbastanza elevato):

![gen forest](./imgs/genforest.png)

dove `p` rappresenta la correlazione media tra gli alberi e `s` misura la forza dell'albero di decision (le performance medie dei classificatori). Pi√π gli alberi diventano correlati (`p` grande) o la forza `s` diminuisce, maggiore sar√† il limite dell'errore (il generalization error aumenta). La correlazione pu√≤ essere migliorata tramite la randomizzazione.

![random forest](./imgs/randomforest.png)

Un vettore casuale pu√≤ essere incorporato nella crescita dell'albero in pi√π modi (come creare il vettore random):

1. **Forest-RI** (random input selection):  vengono selezionate randomicamente `F` features (colonne) tra cui scegliere per effettuare lo split di ogni nodo dell'albero. L'albero viene costruito interamente senza effettuare pruning per ridurre il bias. La forza `s` e la correlazione `p` dipendono da `F`:
   
   - F **piccolo** genera una minore correlazione (la migliora) tra gli alberi ma una minore forza (la peggiora)
   
   - F **grande**: genera una maggiore correlazione (la peggiora) ma una maggiore forza (la migliora)
     
     Un modo (trade-off) per scegliere la dimensione di `F` √® data dalla seguente formula:
     ![trade off](./imgs/tradeoff.png)   dove `d` √® il numero di features.
   
   Dato che non vengono prese in considerazione tutte le features per effettuare gli split, il tempo di runtime √® considerevolmente ridotto.

2. **Forest-RC**: se il numero di feature originale `d` √® troppo piccolo √® difficile scegliere un set di random features indipendente, quindi un modo per risolvere questo problema √® quello di creare nuove combinazioni lineari di feature. Per fare questo, ad ogni split dell'albero vengono prese `L` features di input e vengono combinate tra loro seguendo una distribuzione uniforme ([-1, 1]) e di queste nuove feature viene scelta quella che genera lo split migliore. Anche questa migliora le performance di tempo.

3. **Metodo 3**:  viene selezionata randomicamente una tra le migliori feature di split (le guarda tutte quelle a disposizione). Se le feature a disposizione sono poche pu√≤ generare alberi correlati tra di loro (meglio utilizzare uno degli altri due metodi). Questo metodo non ha il vantaggio di andare a migliorare il tempo di esecuzione perch√© √® costretto a controllare tutte le features ad ogni split.

Le Random Forest sono pi√π robuste al rumore e veloci rispetto all'AdaBoost, sono anche pi√π resistenti all'overfitting dato che hanno alberi pi√π profondi e quindi hanno un bias ridotto.

## Artificial Neural Network (ANN)

Le ANN si ispirano al funzionamento del cervello umano, si basano su:

* Neuroni
* Assoni
* Dendriti
* Sinapsi

Le ANN non hanno tutti questi elementi ma solo i Neuroni (Nodi) e gli Assoni (link pesati) che fungono anche da Dendriti e Sinapsi.

Il modello pi√π semplice di ANN √® chiamato Percettrone e vedremo che sar√† utile per risolvere problemi di classificazione.

### Percettrone (pompotron :robot:)

Il percettrone consiste in 2 tipi di Nodi:

* Pi√π Nodi di Input: che rappresentano i dati di input
* Un Nodo di Output: che rappresenta l'output del modello

I nodi vengono anche chiamati Neuroni o Unit√†.

Ogni nodo input √® connesso con il nodo output tramite un collegamento pesato che emula il collegamento sinaptico. Allenare dunque un percettrone vuol dire aggiustare il valore dei pesi finch√© non si adattano alla relazione di input-output richiesta. Il risultato del neurone di output √® la somma pesata di tutti i neuroni di input pi√π l'aggiunta di un bias (threshold di attivazione)

![percettrone1](./imgs/percettrone1.png)

_Esempio di un percettrone_

Il risultato di un neurone di output pu√≤ essere scritto come:

![output_percettrone](./imgs/output_percettrone.png)

#### Modello di Apprendimento del Pompotron

Come detto prima la fase di Training di un Percettrone vuol dire aggiustare i pesi dei collegamenti. La seguente formula indica come effettivamente viene aggiornato il valore dei pesi dei collegamenti:

![perceptron_learning](./imgs/perceptron_learning.png)

In modo molto intuitivo, il nuovo peso $w^{(k+1)}$ √® la combinazione del vecchio peso $w^{(k)}$ e un valore proporzionale all'errore di predizione $(y - \hat{y})$. Se la predizione √® corretta (il risultato di $(y - \hat{y})$ √® $0$) allora il peso rimane invariato. Altrimenti viene modificato nel seguente modo:

* Se $y = +1$ e $\hat{y} = -1$ : l'errore √® dunque uguale a `2` e per compensare l'errore bisogna aumentare il peso dei link positivi e diminuire il peso dei link negativi.
* Se $y = -1$ e $\hat{y} = +1$: l'errore √® dunque uguale a `-2` e per compensare l'errore bisogna diminuire il peso dei link positivi e aumentare il peso dei link negativi.

Lambda √® chiamato _Learning Rate_, che √® un valore che varia tra 0 e 1 e serve per controllare quanto fini devo essere gli aggiustamenti durante il processo di learning. Se lambda √® pi√π vicino a 0, i nuovi pesi variano meno rispetto a quelli precedenti. Se √® pi√π vicina ad 1, i nuovi pesi possono variare molto rispetto a quelli vecchi. Alcune volte si pu√≤ usare il valore lambda in modo adattivo: all'inizio sar√† pi√π vicino ad 1 in quanto "deve imparare di pi√π" per poi avvicinarsi sempre pi√π allo 0 per effettuare delle piccole modifiche per raggiungere la precisione.

Il percettrone sa fare operazioni di classificazione solo se  i dati sono linearmente separabili, altrimenti √® necessario aumentare la complessit√† del percettrone aggiungendo degli Hidden Layer.

I set di dati linearmente separabili possono essere visti come un Hyperpiano che pu√≤ essere separato da una retta. L'algoritmo di learning del percettrone converge in problemi linearmente separabili, altrove non converge.
La funzione XOR non √® linearmente separabile.

![ipercubo](./imgs/ipercubo.png)

### Esempi di Funzioni di Attivazione

Alcuni esempi di funzioni di attivazione usati negli ANN sono:

- *identity*: viene utilizzata quando il target √® un valore reale (quando si lavora con numeri reali)
- *sign*: si usa su problemi binari
- *sigmoid*: si usa quando si lavora con le probabilit√† perch√© i valori di ritorno sono compresi tra 0 e 1
- *tanh*: simile alla sigmoid ma varia tra -1 e 1. Preferibile alla sigmoid quando gli output richiedono valori sia positivi che negativi
- *ReLU*
- *Hard Tanh*

![funzioni di attivazione](./imgs/attivazione.png)

### MultyLayer ANN

Per creare strutture pi√π complesse per classificare dati non linearmente divisibili si possono utilizzare 2 metodi:

* Il primo √® quello di inserire vari livelli, detti _Hidden Layer_, tra il livello di input e quello di output. La struttura risultante si chiama **MultyLayer Neural Network** pu√≤ essere distinta in base ai link tra i livelli in 2 categorie:
  * *Feed-Forward*, dove i nodi in un livello possono solamente connettersi al livello successivo
  * *Recurrent*, dove i link possono connettere nodi tra lo stesso livello o tra un livello precedente.

![mnn](./imgs/mnn.png)

* Il secondo √® quello di utilizzare funzioni di attivazioni diverse da quella segno, come funzioni lineari, sigmoidi, tangente, ecc..
  Queste funzioni di attivazione permettono ai nodi nascosti e di output di produrre valori di output che hanno valori di input non lineari (come la funzione XOR).
  ![funzioni](./imgs/funzioni.png)

![xor](./imgs/xor.png)

_MNN per classificazione di funzione XOR_

### Learning per ANN

L'obiettivo dell'allenamento di una rete neurale √® quello di determinare un set di pesi che minimizzano la somma degli errori quadratici medi:

![learning task](./imgs/learning.png)

La somma degli errori dipende da `w` perch√© la classe predetta $\hat{y}$ √® una funzione che dipende dai pesi assegnati ai nodi nascosti e quelli di output.

Poich√© spesso l'output di un ANN √® *non lineare* non si riesce a determinare una soluzione per `w` che sia garantita essere globalmente ottimale. Tuttavia sono stati sviluppati dei metodi per aggirare questo problema come il **Gradient Descent**.

La formula per determinare i pesi secondo il gradient descent √® la seguente:

![differenziali](./imgs/differenziale.png)

dove `lambda` rappresenta il learning rate. Il secondo termine √® l'errore che va minimizzato (modifichi i pesi `w` per diminuirlo), tuttavia succede spesso che rimane intrappolato in un minimo locale (perch√© la error function non √® lineare) senza riuscire a trovare il minimo globale. Questo metodo viene utilizzato per trovare i pesi dei nodi di output e di quelli hidden, ma per quest'ultimi risulta molto difficile calcolare il secondo termine dato che non si conoscono i valori di output. Questo problema si risolve tramite la **Back Propagation**: in questo metodo, ogni iterazione dell'algoritmo viene suddivisa in 2 fasi:

1. *Forward*: i pesi ottenuti dalle precedenti iterazioni sono utilizzate per calcolare i valori di output di ogni neurone
2. *Backward*: la formula per l'aggiornamento dei pesi viene applicata al contrario , questo ci permette di usare l'errore dei nodi al layer `k+1` per stimare l'errore dei nodi al layer `k`.

### Convolutional Neural Network

Poich√© in alcuni problemi la posizione del pattern da individuare pu√≤ variare all'interno dell'input √® importante che i modelli siano *Shift Invariant*, ovvero che nel cambiamento della posizione dell'oggetto che si vuole analizzare non deve andare a degradare la capacit√† di classificarlo del classificatore. Di solito nei modelli standard questo non √® possibile a meno che non si creino pi√π modelli che prendono in input varie parti del campione da analizzare, i cui output verranno combinati per individuare il pattern. Tutti i modelli che formano questa mega rete devono essere identici.

![retona](./imgs/retona.png)

Il modo migliore per risolvere questo problema √® di utilizzare le **Convolutional Neural Network**  (CNN) che introducono 2 nuovi tipi di layer che permettono di estrarre le feature latenti presenti nell'immagine (*Convolutional* e *DownSampling*):

- Il layer **Convolutivo**: √® composto da un insieme di kernel/filter (matrici) che vengono fatte scorrere sull'immagine per generare una nuova matrice di output che mette in risalto determinate feature presenti nell'input. Questo layer √® definito dai pesi (valori del filtro), bias e la dimensione dei passi che far√† mentre si sposta lungo l'input (stride). Spesso la dimensione della matrice di output √® minore di quella di input e dunque ci sar√† una perdita di informazione, per questo a volte pu√≤ essere importante aggiungere del padding (tanti 0) attorno all'input originale per mantenere gli output identici. ![filters](./imgs/filters.png) ![padding](./imgs/padding.png)
- Il **Pooling** Layer: serve a sintetizzare l'applicazione del kernel in un numero che viene inserito in una matrice che ha dimensione pari alla dimensione dello stride. ![pooling](./imgs/pooling.png)

Combinando questi due layer con una fully connected network si pu√≤ ottenere una rete convolutiva.

![cnn](./imgs/cnn.png)

### Problemi di Design delle ANN

Quando si sviluppa una ANN bisogna tenere in considerazione questi problemi di design:

* Il numero di nodi di input deve essere determinato, solitamente bisogna creare un nodo di input per ogni variabile, tuttavia, per le variabili categoriche √® accettabile codificarle in una variabile k-array avente `int_sup(log2(k))` nodi di input.

* Il numero di nodi di output deve essere prestabilito: per un problema a 2 classi basta un solo nodo di output, ma per un problema con k classi ne servono k

* Deve essere scelta una topologia per la rete poich√© essa andr√† ad influenzare la target function. Per scegliere la giusta topologia si pu√≤ procedere in 2 modi:
  
  1. Creare una fully connected network e iterarci sopra per costruire una nuova rete ogni volta con un numero minore di nodi (si reitera la procedura di model-building e ha una complessit√† di tempo mooolto alta)
  2. Creare una fully connected network e togliergli nodi per poi ripetere il processo di valutazione della rete.

* Vanno inizializzati i pesi e i bias. E' comunemente accettata una inizializzazione randomica

* Gli esempi di training con valori mancanti dovrebbero essere sostituiti o rimossi

## Cluster Analysis

La Cluster Analysis divide i dati in vari gruppi (clusters) che sono sia Significativi che Utili. In base allo scopo finale della cluster analysis possiamo identificare 2 differenti tipi di analisi:

- **Understanding**: serve per dividere i dati in vari gruppi in base alle caratteristiche che accomunano i dati stessi (serve per identificare le possibili classi per la classificazione in modo automatico).
- **Utility**: approccio che fornisce un'astrazione dei dati individuali all'interno del cluster, generando un **cluster prototype**. Questi cluster prototypes possono essere utilizzati come basi per tecniche di data analysis e data processing.

√à difficile definire come ogni cluster debba essere costituito poich√© per certo dataset esistono molteplici modi  corretti di suddividere i dati in vari cluster (guarda foto sotto). L'unico modo per capire qual √® la corretta suddivisione in cluster e' analizzando i dati.

_N.B il clustering pu√≤ sembrare simile alla classificazione ma risulta essere privo di fase di training supervisionato._

![cluster esempio](./imgs/cluster.png)

### Tipi di clustering

Il clustering √® la collezione di un insieme di cluster. Possono essere suddivisi in base ad alcune caratteristiche che possono presentare:

#### Hierarchical vs Partitional

- _hierarchical_: suddivide i dati in cluster nestati che possono essere rappresentati con un albero in cui la radice rappresenta la totalit√† del dataset e, pi√π si va in profondit√†, pi√π aumenter√† il numero di cluster e diminuir√† il numero di record.
- *partitional*: i dati vengono suddivisi in maniera "standard", dunque non sono ammessi sotto cluster all'interno di ogni cluster. Ogni layer dell'albero di un Hierarchical Clustering pu√≤ essere visto come un Partitional Clustering diverso.

#### Exclusive vs Overlapping vs Fuzzy

- *exclusive*: ogni data object pu√≤ appartenere ad un solo cluster
- _overlapping_: ogni data object pu√≤ appartenere a pi√π cluster simultaneamente. Questo pu√≤ essere utili per dati che possono essere identificati in pi√π classi (uno studente universitario pu√≤ anche essere un dipendente dell'universit√†)
- _fuzzy_: ogni data object appartiene a tutti i cluster con un peso di appartenenza (membership) che varia tra `0` e `1` dove `0` rappresenta la NON appartenenza e `1` la totale appartenenza. Sostanzialmente crea una distribuzione di probabilit√† dell'appartenenza dei dati alle varie classi (√® esclusivo, ogni dato pu√≤ appartenere solo ad un cluster).

#### Complete vs Partial

- *complete*: assegna tutti i data object del data ad un cluster senza lasciarne nessuno fuori
- *partial*: esclude dai cluster alcuni elementi che non mostrano appartenenza a nessun cluster (possono essere punti di *rumore* o *outliers*)

### Tipi di Cluster

- *Well Separated*: sono dati ben divisi tra di loro (appiano naturalmente separati e non globulari). Ogni dato del cluster √® pi√π vicini ai punti del cluster a cui appartiene che ai punti di altri cluster.
- Prototype Based: si basa sul concetto di Prototype (la generalizzazione del Cluster), ogni data object √® pi√π vicino al Prototype del cluster di appartenenza che del Prototype di altri cluster. Per attributi continui, il prototype del cluster √® il **Centroid** (la media); per attributi categorici √® il **Medoid** (il punto pi√π rappresentativo). Viene anche chiamato **Center Based**.
- Graph Based: i dati sono raggruppati in base alle connessioni che hanno sul grafo che li rappresenta. Un cluster √® un insieme di punti connessi tra di loro che non sono connessi con altri elementi. √à suscettibile al rumore in quanto un punto pu√≤ creare ponti tra cluster differenti ed unirli.
- Density Based: guarda la densit√† dei punti. Un cluster √® un insieme denso di punti circondato da un insieme con scarsa densit√†. √à pi√π resistente al rumore del Graph Based.
- Shared Property: un cluster √® formato da data object che hanno alcune caratteristiche in comune

![tipi di cluster](./imgs/tipi_clusters.png)

### Clustering Algorithms

#### K-means

Questo algoritmo √® una tecnica di **Partitional Clustering** basata su **Prototype-Based Cluster**. Funziona cercando di trovare, dato un numero di Centroidi definito dall'utente `K`, i punti che sono pi√π vicini ad ogni centroide e che andranno quindi ad identificare un cluster (simile al knn).

Ci sono due variazioni di questo algoritmo, a seconda del tipo di dato con cui lavoriamo:

- K-means per dati continui
- K-medoid per dati categorici

##### Algoritmo

1. Si selezionano `K` punti come Centroidi (punti a caso nel dataset)
2. Si assegna ogni punto del dataset al suo centroide pi√π vicino e si ricalcolano i centroidi dei nuovi cluster cos√¨ generati
3. Si continua fin quando i centroidi non cambiano pi√π

Spesso la condizione `3.` genera dei punti che oscillano tra cluster differenti impedendo che l'algoritmo converga in una soluzione, dunque √® possibile rilassare questa condizione rimpiazzandola con la seguente:

```
si continua fino a quando solo l'1% dei punti cambia cluster
```

A seconda del tipo di spazio di dati con cui si lavora possono essere utilizzate varie misure di prossimit√† per calcolare la distanza punti-centroidi. Alcuni esempi sono:

- **Euclidea (L2)** o **Manhattan (L1)** per gli spazi Euclidei
- **Cosine Similarity** o la **Jaccard Measure** per i documenti

Per scegliere i Centroidi migliori ottenuti con varie esecuzioni del K-means √® necessario scegliere un objective function adeguata. Per farlo √® necessario calcolare la qualit√† del Clustering utilizzando la Somma dell'Errore Quadratico (SSE). Guardando questo indice, dati due cluster ottenuti con K-means, quello che avr√† SSE minore sar√† il migliore. Per calcolare l'SSE con la distanza Euclidea si usa la seguente formula:

![sse](./imgs/sse.png)

La formula che segue √® come vengono aggiornati i centroidi all'i-esima iterazione

![ci](./imgs/sse1.png)

Con vari calcoli, utilizzando la distanza euclide, si pu√≤ dimostrare che il centroide migliore √® la media.

![basic](./imgs/basic.png)

Inizializzare in modo Random i Centroidi pu√≤ degradare la qualit√† dei Cluster creati dato che non √® garantita la convergenza ad un minimo globale. Un approccio naive per risolvere questo problema potrebbe essere quello eseguire pi√π volte il K-means scegliendo ogni volta Centroidi di partenza diversi ed infine scegliendo il Clustering con SSE minore. Risulta essere molto costoso e non garantisce il ritrovamento di una soluzione. Un altro metodo √® di creare dei Cluster di tipo Hierarchical campionando dei punti dal dataset ed utilizzarli per calcolare i Centroidi che verranno poi utilizzati come centroidi iniziali. Questo approccio √® utilizzabile solo se i campioni sono piccoli (Hierarchical Clustering √® costoso) e `K` deve essere piccolo rispetto alla dimensione dei campioni.

##### Costo

- Spazio: $O((m+K)n)$ dove $m$ √® il numero di data point, $n$ √® il numero di attributi
- Tempo: $O(I \cdot K \cdot m \cdot n)$ dove $I$ √® il numero di iterazioni richieste per la convergenza

`I` pu√≤ essere limitata superiormente dato che tendenzialmente i cambiamenti pi√π significativi avvengono nelle prime iterazioni. L'algoritmo risulta efficiente e semplice a patto che `K` sia significativamente minore di `m`.

##### Bisecting K-Means

Il funzionamento √® il seguente:

1. Considera tutti i dati come un unico mega cluster
2. Applica 2-means (k-means con 2 centroidi) per ottenere 2 sotto cluster
3. Scegli il sotto cluster da suddividere basandosi su criteri come numero di elementi (splitti quello pi√π grosso) o SSE (splitti quello con SSE maggiore)
4. Ripetere step 2. e 3. fino al numero desiderato di cluster

![bisect](./imgs/bisect.png)

##### Applicabilit√†

K-means si applica a cluster globulari, ben separabili e con dimensioni e densit√† simili. Per K abbastanza grande pu√≤ essere applicato a cluster naturali ed √® in grado di trovare sotto cluster puri. √à molto suscettibile agli outliers e la loro detection e rimozione pu√≤ essere molto utile.

![applicabilit√†](./imgs/applicabilita.png)

#### Agglomerative Hierarchical

Queste tecniche sono utilizzate per generare cluster di tipo Hierarchical e ne esistono 2 diversi tipi:

- **Agglomerative**: all'inizio ogni punto √® un cluster e procedendo con l'algoritmo, punti vicini vengono fusi insieme fino ad ottenere un unico mega cluster. √à necessario definire il concetto di 'prossimit√†'.
- **Divisive**: inizia con un cluster contenente tutti i punti che mano a mano viene diviso, fino ad ottenere cluster singleton. Serve una tecnica per decidere chi e come splittare.

√à possibile rappresentare Clustering di questo tipo con un diagramma simile ad un albero chiamato _Dendrogramma_.

![dendrogram](./imgs/dendo.png)

##### Basic algorithm

1. Calcola la proximity matrix (se necessario)
2. Unisci i due cluster pi√π vicini
3. Aggiorna la proximity matrix
4. Ripeti i punti 2. e 3. fin quando non rimane un solo cluster

Ci√≤ che caratterizza questi algoritmi di Clustering √® il metodo con cui viene definita la _prossimit√†_. I due approcci principali sono:

- Graph Based: Si basa su un'astrazione del cluster che viene visto come un Grafo. Per questa tecnica si hanno varie implementazioni:
  
  - **MIN**: calcola la prossimit√† in funzione della distanza tra i punti pi√π VICINI di cluster differenti (aka single link). Questa tecnica √® buona per gestire cluster dalla forma non-ellittica, ma molto sensibile a rumore e punti di outlier.
  - **MAX**: calcola la prossimit√† in funzione della distanza tra i punti pi√π LONTANI di cluster differenti (aka complete link). Risulta pi√π resistente al rumore ed agli outliers ma pu√≤ spezzare cluster grandi favorendo forme globulari.
  - **GROUP AVERAGE**: calcola la media delle distanze tra tutti i punti di due cluster differenti. Questo approccio √® un compromesso tra il MIN e il MAX. √à meno suscettibile al rumore ma predilige forme globulari.![graph based](./imgs/graphbased.png)

- Prototype Based: basa il calcolo della prossimit√† sui centroidi (che rappresentano il cluster).
  
  - **Centroid Method**: basa il calcolo della prossimit√† sulla distanza tra i centroidi di differenti cluster (forse deve essere minima). Questo metodo presenta un problema che non √® presente in nessun altro metodo Hierarchical: l'**inversione**, in cui due cluster che vengono fusi possono essere pi√π simili di un paio di cluster fusi in precedenza.
  - **Ward's Method**: aggiunge al calcolo, oltre all'uso dei centroidi, il concetto di SSE che deve risultare minia quando vengono fusi due cluster. Questa tecnica √® meno suscettibile al rumore, ma favoreggia cluster di forma globulare. Utilizza la stessa objective function del K-means ('√® l'equivalente gerarchico del K-means').

##### Complessit√†

La complessit√† in spazio √®: $O(m^2)$.

La complessit√† in tempo √®: $O(m^3)$.

Va notato che questa pu√≤ essere ridotta se si utilizzano liste ordinate o heap per tenere traccia dei dati. Questo riduce la complessit√† in tempo a: $O(m^2 \ log(m))$.

Questi costi molto elevati rendono la scalabilit√† di questi tipi di clustering molto difficile.

##### Forza e Punti Deboli

1. **Mancanza di una objective function globale**: le tecniche appena viste decidono localmente il processo di ottimizzazione. Questo √® uno svantaggio perch√© non ci sar√† un processo di ottimizzazione globale, per√≤ semplifica anche la rivoluzione del problema. Per via della sua complessit√† in spazio e in tempo molti dataset non sono risolvibili. Questo problema dell'ottimizzazione deriva dal fatto che una volta effettuata l'operazione di merge essa non potr√† essere annullata. Una possibile soluzione a questo problema di non reversibilit√† √® quello di provare a spostare i rami dell'albero generato per provare a migliorare la global objective function; un altro metodo ancora √® quello di utilizzare un algoritmo come K-means per generare molti piccoli cluster che verranno utilizzati come punto di partenza dall'algoritmo di hierarchal clustering.
2. Sono suscettibili al rumore
3. Presentando difficolt√† nel gestire cluster di diverse dimensioni e di forma non globulare
4. Dividono cluster grandi in cluster pi√π piccoli

#### DBScan

DBScan √® un algoritmo di Clustering che si basa sul concetto di densit√†: per uno specifico punto √® la quantit√† di punti vicini ad esso compresi in un dato raggio (definito dall'utente) _EPS_ (questo include anche il punto stesso !). Questo metodo √® abbastanza semplice da implementare ma la scelta del raggio risulta critica, per un EPS abbastanza grande possiamo avere, come densit√† di un punto, `m` (il numero dei punti nel nostro dataset) e per un raggio sufficientemente piccolo riusciamo a trovare come densit√† 1. L'algoritmo DBScan cerca di trovare un modo per la scelta di un EPS adeguato.

In base al punto in cui si trovano, i data point di un Clustering di tipo Center-Based possono essere classificati nel seguente modo:

- **Core Point**: punti interni ad un density based cluster sono quei punti che ricadono all'interno di un raggio specifico (_EPS_) e superano una certa condizione _MinPts_. Sia EPS che MinPTS sono scelti dall'utente.
- **Border Point**: sono quei punti che non sono Core Point, ma che ricadono all'interno di un vicinato di un Core Point. Un Border Point pu√≤ appartenere a diversi vicinati di Core Point diversi
- **Noise Point**: sono quei punti che non sono n√® Core Point n√® Border Point.

Nella figura sottostante possiamo vedere che, dato un EPS e MinPts <= 7, il Punto `A` risulta essere un Core Point (ha 7 punti nel suo vicinato e quindi supera la condizione di MinPts); il punto `B` non soddisfa la condizione MinPts ma ricade all'interno di un vicinato (quello del punto A) quindi √® un Border Point; `C` non √® n√® un core point n√® un border point quindi √® un Noise Point.

![raggio](./imgs/raggio.png)

##### Algoritmo

1. Etichetta tutti i punti come **Core**, **Border** o **Noise**
2. Elimina i Noise Point
3. Raggruppa tutti i Core Point che sono all'interno di un raggio _EPS_ gli uni degli altri
4. Crea un cluster con ogni gruppo di Core Point creato allo step 3.
5. Assegna ogni border point ad uno dei suoi core point associati 

Il problema principale di questo algoritmo √® quello di selezionare un valore appropriato per _EPS_ e _MinPts_. L'approccio base per trovarli √® quello di guardare come varia la distanza tra i punti ed i loro k-esimi vicini pi√π vicini (k-dist). Per punti che appartengono ad un cluster, k-dist sar√† piccola (alta densit√†); mentre per Noise Point sar√† grande (bassa densit√†). √à dunque possibile stimare i parametri calcolando la k-dist per ogni punto del dataset, ordinarli in ordine crescente e vedere il punto in cui c'√® la variazione pi√π netta (il momento di transizione tra elementi appartenenti ad un cluster e rumore). Questo valore verr√† utilizzato com EPS ed il valore k (utilizzato nella k-dist) verr√† assegnato a MinPts. Il valore EPS dipende dalla scelta di `k`, ma generalmente non cambia poi cos√¨ tanto al variare di k. Se `k` viene scelto troppo piccolo, allora anche alcuni Noise Point verranno inseriti nei cluster; invece con `k` troppo grande, cluster di piccole dimensioni verranno etichettati come rumore. DBScan originale utilizza `k = 4` dato che funziona generalmente bene per la maggior parte dei dataset di 2 dimensioni.

![noice](./imgs/noice.png)

##### Complessit√† in Spazio e Tempo

La complessit√† in spazio di questo algoritmo √® $O(m)$ in quanto deve salvare in memoria solo poche informazioni (l'etichetta di ogni punto: Core, Noise, Border ed il cluster label).

La complessit√† in tempo √®, nel caso peggiore $O(m^2)$, ma tramite l'utilizzo di strutture dati come i kd-tree (solo nel caso di dataset con spazio a bassa dimensione), riesce a scendere fino a $O(m \ log(m))$.

##### Vantaggi e Svantaggi

- Pu√≤ trovare cluster con forme che non potrebbero essere trovate da nessun altro algoritmo
- Se la densit√† dei punti del dataset sono estremamente variabili non √® garantito il ritrovamento di una soluzione corretta
- √à resistente al rumore
- Poco applicabile quando si lavora con alte dimensionalit√†
- Se non √® possibile calcolare i vicini pi√π vicini utilizzando strutture dati particolari, l'algoritmo pu√≤ risultare costoso (generalmente succede in dataset ad alte dimensioni)

![dbscan](./imgs/dbscan.png)

### Cluster Evaluation

A volte pu√≤ essere utile valutare i risultati forniti da un algoritmo di Clustering allo stesso modo in cui viene valutato un modello di classificazione. Spesso non √® necessario e non √® facile da applicare dato che ci sono vari algoritmi con funzionamenti diversi e per ogni caso servirebbero metodi e metriche diverse. Gli algoritmi di clustering trovano sempre cluster anche se effettivamente non esistono cluster naturali nei dati, quindi risulta utile controllare se quei cluster sono sensati (in dati con alte dimensioni non √® facile individuare visivamente questa problematica).

![validation](./imgs/clustervalidation.png)

Gli indici di valutazione utilizzati per valutare vari aspetti dei cluster sono suddivise nelle seguenti categorie:

- **Unsupervised**: misura varia aspetti della struttura del cluster senza basarsi su dati esterni (un esempio √® SSE), spesso sono chiamati **Internal Indices**. Ne esistono 2 sotto categorie:
  - _Cluster Cohesion_: determina quanto gli oggetti del cluster sono correlati tra di loro
  - _Cluster Separation_: determina quanto, cluster differenti, sono separati o ben distinti l'uno dall'altro
- **Supervised**: misura quanto le strutture generate da un algoritmo di clustering, corrispondono ad una qualche struttura esterna (un esempio √® l'Entropia). Spesso queste misure sono chiamate **External Indices**.
- **Relative**: misura che serve per confrontare diversi clustering o cluster tra di loro. Pu√≤ essere sia Supervised che Unsupervised. Un esempio pu√≤ essere l'SSE per l'unsupervised e l'entropy per la supervised.

#### Unsupervised Cohesion and Separation

La validit√† di un cluster per un insieme di K cluster, in generale pu√≤ essere espressa come la somma pesata della validit√† dei singoli cluster:

![coesione](./imgs/coesione.png)

La funzione `validity` pu√≤ essere sia Coesione, Separazione o una combinazione delle due. I pesi `w` dipendono da caratteristiche del cluster: potrebbero essere tutti 1, la radice quadrata della coesione, la dimensione del cluster, ecc.
Se, per la validity function si sceglie la coesione, valori grandi sono meglio; se viene scelta la separazione, valori pi√π piccoli sono meglio.

##### Graph Based

Per i graph based cluster, la Coesione e la Separazione vengono espressi nel seguente modo:

- Cohesion: √® la somma dei pesi dei cammini nel proximity graph che connette punti nello stesso cluster ![formula](./imgs/coesione_formula.png)
- Separation: √® la somma dei pesi dei cammini dai punti di un cluster ai punti di un altro cluster. ![separazione](./imgs/separazione.png)

![separation coesion](./imgs/separazioncoesion.png)

##### Prototype Based

Per i cluster prototype based la coesione e la separazione  si esprimono nel seguente modo:

- Cohesion: √® definita come la somma delle prossimit√† tra il prototipo di un cluster (centroide/medoide) ed i suoi punti. ![proto coesione](./imgs/coesioneproto.png)
- Separation: √® data dalla misura della prossimit√† di prototipi di due cluster differenti. ![separation proto](./imgs/separation_proto.png)

![proto seperation coesion](./imgs/protoseparationcoesion.png)

##### Overall

Questi due indici possono essere misurati in vari modi in base a come viene calcolato il peso. Questi sono alcuni esempi:

![overall](./imgs/overoll.png)

##### Relazione tra Coesione e Separation

La Coesione e la Separazione sono, in alcuni casi, fortemente correlate tra di loro, infatti √® possibile dimostrare (noi non lo faremo) che la somma tra SSE Totale e SSB Totale √® costante, ergo massimizzare l'SSB (separazione) equivale a minimizzare l'SSE (Coesione).

##### Silhouette Coefficient

Metodo per la valutazione di un singolo Cluster che combina i concetti di Coesione e Separazione. Si calcola come segue:

1. Per l'i-esimo oggetto (punto) se ne calcola la distanza media tra tutti gli altri oggetti dello stesso cluster a cui appartiene. Il risultato di questo step viene chiamato $a_i$
2. Per l'i-esimo oggetto e per ogni cluster non contenente l'oggetto calcolare la distanza media tra tutti gli oggetti in uno dei questi cluster (quelli che non contengono l'oggetto). Dopodich√© si prende il valore minore tra queste distanze che chiameremo $b_i$.
3. Per l'i-esimo oggetto il Silhouette Coefficient √® $s_i = \frac{b_i - a_i}{ max(a_i, b_i)}$

![silouette](./imgs/silouette.png)

$s_i$ pu√≤ variare tra $-1$ e $1$. Il valore $1$ √® il migliore (si ottiene solo quando $a_i = 0$) mentre $-1$ √® un valore brutto perch√©, in questo caso, la distanza $a_i$ risulta pi√π grade di $b_i$, vuol dire che il punto analizzato apparterrebbe pi√π ad un cluster che non lo contiene piuttosto che a quello che lo contiene.

Questo coefficiente pu√≤ essere utilizzato per misurare la bont√† di un clustering calcolandolo su tutti i punti e poi facendo una media.

##### Unsupervised Similarity Matrix

Per giudicare la bont√† di un clustering possiamo anche utilizzare un approccio grafico che si basa sulle matrici. √à possibile farlo misurando la correlazione tra la similarity matrix e una similarity matrix ideale calcolata basandosi sui label del dataset, se queste due matrici si assomigliano possiamo dire che il clustering √® buono. √à possibile esprimere un giudizio sulla bont√† di un clustering anche ad occhio nudo osservando la similarity matrix: una matrice ``n x n`` dove `n` √® il numero di punti del dataset, la i-esima cella conterr√† il valore della similarit√† (varia tra 0 e 1) tra i due punti che la identificano. Le righe e colonne di questa matrice verranno poi ordinate in modo tale da avere punti appartenenti allo stesso cluster tutti vicini. Nella matrice ideale, tutti i punti che appartengono allo stesso cluster avranno 1, mentre gli altri 0 e si formeranno blocchi ben definiti sulla diagonale che rappresenteranno i cluster trovati.

![sim matr](./imgs/similaritymatrix.png)
_Esempio di buona similarity matrix_

![cattiva sim](./imgs/cattivasim.png)
_Esempio di similarity matrix su dati random (no real clusters)_

##### Giusto numero di cluster

Per decidere qual √® il giusto numero di cluster in cui dividere un dataset, bisogna analizzare le curve formate dagli indici di SSE o di Silhouette Coefficient:

- per SSE si guarda dove viene creato un 'gomito'
- per Silhouette si guarda dove compare un picco

![bello bello](./imgs/bellobello.png)

#### Supervised Measures

Quando si analizzano cluster con approccio supervised possiamo identificare due tecniche differenti: 

- classification based: si basa su metodi per valutare i classificatori gi√† visti in precedenza (Entropy, Precision, Purity, Recall, F-measure)
- similarity based

Lo scopo di questo tipo di approccio √® quello di vedere quanto una tecnica automatica pu√≤ produrre risultati comparabili ad un'analisi manuale.

#### Assessing the Significance of Cluster Validity Measures

Spesso, una volta ottenuto un valore dall'indice di misura utilizzato per valutare un cluster/clustering sar√† necessario anche fornirne un'interpretazione. Per alcuni indici, come Purity o Entropy, si pu√≤ utilizzare la definizione della misura per capire se il risultato ottenuto √® buono o meno (`Entropy = 0` molto cattivo). Per tutte le altre si pu√≤ utilizzare un approccio statistico: 

1. si calcola una distribuzione della misura di nostro interesse basandosi sui dati
2. si calcola l'effettivo valore per i cluster ottenuti
3. si valuta quanto √® probabile che la misura ottenuta sia un risultato puramente casuale. Se questa probabilit√† √® bassa, allora la misura √® buona.

![ultimo](./imgs/ultimocluster.png)

La precedente immagine √® ottenuta campionando un dataset con cui √® stato generato un cluster 3-mean. Mostra la distribuzione random dell'SSE dati 500 campioni. Per interpretare se l'SSE del cluster originale √® buono o no, si guarda se il valore che assume cade o no all'interno dell'istogramma: se ci cade dentro √® probabile che sia frutto di casualit√† (non va bene), se non ci cade allora √® abbastanza probabile che sia effettivamente un buon indice.

## Anomaly Detection

Spesso in un dataset sono presenti dati anomali che per√≤ possono avere un'importanza significativa, sono chiamati _outliers_. Pu√≤ essere di interesse la loro individuazione e la branca che studia come individuarle √® chiamata Anomaly Detection. Storicamente √® stata studiata per rimuovere dati anomali che potevano interferire con l'allenamento di un dato modello, esecuzione di algoritmi di clustering, ecc. Spesso questa √® parte del preprocessing.

Alcuni esempi di applicazione dell'anomaly detection sono:

- Fraud Detection
- Intrusion Detection
- Ecosystem Disturbances
- Public Health
- Medicine üßë‚Äçü¶º

### Cause delle Anomalie

Un outlier (anomalia) √® definito come segue (Definizione di Douglas Howking (Hawkins) Mortimer Giunior II(2)): un outlier √® un osservazione che differisce talmente tanto dalle altre osservazioni che fa sorgere il sospetto (sus) che sia stata generata da un meccanismo differente.

Le anomalie possono essere generate da differenti cause. Di seguito illustreremo le principali:

- **Data from different classes**: un oggetto pu√≤ essere diverso dagli altri oggetti, e quindi anomalo, poich√© appartiene ad una classe di diverso tipo. Un esempio √® una persona che usa la carta di credito in modo fraudolento, apparterr√† ad una classe differente rispetto ad una che ne fa un uso normale. Questa classe di anomalie √® il focus dell'anomaly detection nel data mining.
- **Natural Variation**: spesso i dataset assumono distribuzioni che si possono ricondurre a distribuzioni statistiche ben conosciute (come la normale) e in queste distribuzioni la maggior parte die dati √® concentrata intorno alla media, dunque dati anomali saranno quelli che una o pi√π attributi assumono valori che si discostano, anche di molto, dalla media (dal centro). Un esempio √® l'altezza in cui una persona molto alta far√† sempre parte della stessa classe delle altre, ma avr√† il valore dell'altezza che varia di molto rispetto alla media generale.
- **Data Measurement and Collection Errors**: Spesso quando vengono raccolti i dati si possono generare errori causati o dallo strumento con cui si raccolgono o dall'errore umano. Si andranno dunque a generare delle anomalie che non sono desiderabili, dato che vanno a peggiorare la qualit√† del dataset. Dunque queste anomalie vanno eliminate e sono il focus del preprocessing e nello specifico del _data cleaning_.

### Differenti Approcci

Una distinzione ad alto livello tra gli approcci per la anomaly detection pu√≤ essere la seguente:

- **Model Based**: prima viene generato un modello partendo dai dati e vengono considerati anomali tutti i dati che non vengono riconosciuti dal modello (do not fit the model). Un esempio pu√≤ essere la distribuzione creata stimando statisticamente i parametri. Questa tecnica pu√≤ essere fatta sia con modelli di classificazione che regressione. Per la regressione un oggetto √® un'anomalia se il suo valore √® molto lontano da quello predetto. Per la classificazione si possono considerare 2 classi, una per i dati anomali ed una per quelli normali e procedere con la classificazione (servono sempre le label perch√© senno non possiamo creare un modello).
- **Proximity Based**: vengono considerati anomali gli oggetti che distano maggiormente dalla maggioranza degli altri oggetti. Questo tipo di tecnica permette una visualizzazione grafica semplice delle anomalie (quando i dati sono 2 o 3 dimensionali) utilizzano degli scatter plot e individuando i punti maggiormente separati dagli altri.
- **Density Based**: oggetti che si trovano in regioni a bassa densit√† sono relativamente distanti dai loro vicini e dunque possono essere considerati anomali. Un'accortezza in pi√π √® quella di classificare come outlier i punti solo se hanno densit√† locale significativamente minore della maggior parte dei propri vicini (questo approccio evita di classificare erroneamente aree di minor densit√† del dataset che per√≤ presentano valori validi).

Questi approcci, in base alla conoscenza che si ha dei dati, possiamo dividerli in 3 categorie:

- **Supervised**: si ha un dataset con oggetti normali e outlier che hanno entrambi label che li identificano. Sar√† dunque possibile allenare un modello in gradi di identificare le anomalie.
- **Unsupervised**: non c'√® disponibilit√† di class label e dunque l'obbiettivo sar√† quello di assegnare un punteggio ad ogni valore che andr√† a riflettere quanto esso viene considerato anomalo. Tuttavia se le anomalie sono simili tra di loro, andranno a fare abbassare questo valore e dunque a farle riconoscere come oggetti normali.
- **Semisupervised**: i label sono presenti solo per gli oggetti normali, dunque l'obiettivo sar√† analogo all'unsupervised per√≤ si avr√† una maggiore resistenza alle anomalie simili.

#### Approccio Statistico

Un esempio di _model based_ approach √® lo Statistical Approach. Tale approccio crea un modello statistico stimandone i parametri dal dataset di partenza e gli outlier vengono identificati in base alla loro probabilit√† dato il modello scelto.

**Definizione probabilistica di outlier**: un outlier √® un oggetto che ha una bassa probabilit√† rispetto alla probability distribution utilizzata per modellare i dati.

##### Problematiche

- _Identificazione della giusta distribuzione_: spesso non √® facile identificare la distribuzione che meglio rappresenta i dati e questo pu√≤ portare a delle classificazioni errate.
- _Numero di attributi usato_: le anomalie si possono presentare su uno o pi√π degli attributi dei dati di interesse, dunque se un dato attributo non √® anomalo non significa che quel dato non lo sia. √à importante scegliere il giusto numero di attributi da analizzare a seconda dei dati che si hanno.
- _Mischiaticcio di distribuzioni_: i dati possono essere modellati da una misticanza di distribuzioni, bench√© pi√π potente risulta essere pi√π complicata sia da individuare che da utilizzare.

##### Distribuzione Normale Univariata

_Dati univariati = dati osservati in un solo attributo._

Una delle distribuzioni pi√π versatili √® quella Normale che √® in funzioni dei parametri $\mu$ e $\sigma$ ($N(\mu, \sigma)$). Questa pu√≤ essere utilizzata per lo scopo di anomaly detection nel seguente modo:

**Definizione di outlier per un singolo attributo N(0,1)**: un oggetto con attribute value `x` dalla distribuzione gaussiana `N(0,1)` √® un outlier se `|x| >= c`, dove `c` √® una costante scelta in maniera tale che `prob(|x|) >= c = a`.

`a` √® una costante che va scelta per far funzionare la definizione rappresenta il grado di rarit√† dell'oggetto `x` (quanto √® improbabile che appartenga alla distribuzione). Molto probabilmente non avremmo mai una distribuzione `N(0,1)` e dunque dovremmo trovare un modo per trasformare l'attributo `x` in un nuovo attributo `z` che abbia la distribuzione `N(0,1)`. Per farlo dobbiamo stimare i parametri $\mu$ e $\sigma$ tramite l'utilizzo della media campionaria e la deviazione standard campionaria. Questo approccio funziona bene quando abbiamo molti dati. Molto spesso per√≤ la distribuzione stimata non √® proprio `N(0,1)` e  per risolver questo problema c'√® il metodo di Grubb.

##### Distribuzione Normale Multivariata

Possiamo vedere il nostro dataset come mix di distribuzioni di probabilit√†:

- M che √® quella dei punti normali
- A che √® quella degli outliers

Queste distribuzioni vanno scelte e di solito per gli outlier si utilizza una distribuzione normale. 

Un metodo basato su questo concetto e quello della Verosimiglianza (likelihood), quanto una distribuzione riesce ad approssimare un dataset, per effettuare Anomaly Detection √® il seguente:

1. Assumere che tutti i punti appartengono a M e A √® vuoto
2. Calcolare la verosimiglianza del dataset alla distribuzione scelta
3. Provare a spostare un punto da M ad A e vedere se la verosimiglianza aumenta: se lo fa quel punto viene messo permanentemente in A (quindi un outlier)
4. Ripeter per tutti i punti del dataset

##### Pro e Contro

- Si basano sulla teoria di base della statistica (buono perch√© sai che questa teoria funziona)
- Quando c'√® abbastanza conoscenza dei dati e dei tipi di test da applicare, ha un'alta efficienza
- Ci sono un'ampia variet√† di test per attributi singoli, meno per opzioni per quelli multivariati
- Scarse performance per dati multidimensionali

#### Approccio Proximity Based

√à un approccio pi√π facile ed intuitivo di quello statistico dato che √® molto pi√π facile determinare una misura di prossimit√† significativa rispetto al determinare una distribuzione statistica di un dato dataset. Il metodo pi√π semplice per valutare la distanza √® quello di k-nearest neighbor, in cui ad ogni punto viene assegnato un valore che riflette quanto esso sia un outlier oppure no. Questo valore √® 0 per punti normali e infinito per punti che sono sicuramente outlier.

**Definizione di outlier**: l'outlier score di un oggetto √® dato dalla distanza dal suo knn.

√à importante scegliere correttamente il numero `k` poich√© se si sceglie `k` grande quasi quanto la dimensione del dataset allora tutti i valori di quel dataset verranno considerati outlier, mentre se √® troppo piccolo alcuni outlier verranno considerati normali. Per ridurre il problema della scelta di k √® possibile utilizzare al posto della distanza dal suo knn la media tra le distanze dei primi knn.

##### Pro e Contro

- Semplice
- Costoso e poco applicabile per dataset grandi ($O(m^2)$ in tempo)
- Sensibile alla scelta dei parametri
- Non √® in grado di gestire dataset con regioni a densit√† variabili (utilizza threshold globali che non tengono conto di variazioni di densit√†)

![outlier](./imgs/outlier.png)

In questo esempio possiamo vedere come scegliendo `k` troppo grande i pochi punti in alto a destra che formano un cluster naturale vengono visti come outlier.

#### Approccio Density Based

Gli outlier per un approccio density based sono definiti nel seguente modo: l'outlier score di un oggetto √® l'inverso della densit√† di quell'oggetto, dunque questo approccio basa il suo funzionamento sulla densit√† di una data area intorno ad un punto. √à facilmente intuibile che √® strettamente correlato all'approccio Proximity Based poich√© la densit√† √® un concetto che deriva dalla prossimit√†.

La densit√† √® definita dalla seguente formula: ![densit√†](./imgs/densita.png) dove `N(x,k)` √® un insieme che contiene i k vicini pi√π vicini di x; `|N(x,k)|` √® la dimensione dell'insieme; `y` √® il vicino pi√π vicino.

Un altra definizione di densit√† pu√≤ essere la seguente: la densit√† attorno ad un oggetto equivale al numero di oggetti che si trovano all'interno di una distanza specificata `d` dall'oggetto (come il knn). Il parametro `d` deve essere scelto con cura perch√© valori troppo grandi falliranno nell''identificare gli outlier e valori troppo piccoli identificheranno punti normali come outliers.

La stessa problematica del proximity based si presenta anche in questo approccio, non √® in grado di gestire dataset con aree di densit√† variabile. Ci sta un metodo per aggirare il problema: invece di considerare la densit√† assoluta si considera solo la densit√† relativa di un dato punto che pu√≤ essere trovata con la seguente formula: ![densit√† relatia](./imgs/densitarelativa.png). Un algoritmo che usa questo approccio prende il nome di Local Outlier Factor (**LOF**) di cui vedremo una versione semplificata:

1. Si itera per ogni `x` appartenente al dataset e si determinano i suoi k vicini pi√π vicini
2. Si calcola il valore `density(x,k)` per ogni `x` utilizzando i suoi `k` vicini pi√π vicini (punto 1.)
3. Per ogni `x` assegna un outlier score utilizzando l'equazione sopra riportata (average relative density)

![lof](./imgs/lof.png)

##### Pro e Contro

- Se ci si basa sulla relative density si possono gestire anche aree di densit√† variabile
- Complessit√† in tempo elevata: $O(m^2)$. Pu√≤ essere ridotta a $O(m \ log(m))$ per dati a basse dimensioni utilizzano strutture dati speciali
- La scelta dei parametri √® difficili

#### Approccio Clustering Based

Poich√© gli algoritmi di clustering trovano quanto un dato insieme di punti √® correlato con altri punti viene intuitivo capire che questi algoritmi possono essere anche utilizzati per determinare l'inverso: quanto un punto si discosta notevolmente dagli altri. Un approccio per effettuare anomaly detection con clustering √® quello di scartare piccoli cluster che sono lontani dagli altri cluster. Per questo approccio √® necessario stimare dei threshold minimi per la dimensione del cluster e la distanza. Un approccio pi√π sistematico √® quello di determinare quanto ogni punto appartiene ad un dato cluster (per prototype based la distanza dai centroidi, oppure quanto un punto peggiora la objective function).

**Definizione di cluster based outlier**: un oggetto √® un cluster based outlier se essono non appartiene fortemente a nessun cluster.

##### Determinazione di quanto un oggetto appartiene ad un cluster

Come detto in precedenza, per determinare un outlier score si possono applicare vari metodi. Per prototype based √® possibile calcolare la distanza da il prototype e il oggetto, tuttavia questo metodo non funziona bene per cluster con densit√† variabili e quindi in questi casi l'outlier score pu√≤ essere determinato con la distanza relativa tra il prototype e tutti gli altri oggetti. Se i cluster possono essere modellati in termini di distribuzioni gaussiane utilizziamo la distanza di Mahalanobis üå¥ ( aka Bionicle Divinit√†).

Per tecniche di clustering che hanno un objective function possiamo assegnare come outlier score dell'oggetto il miglioramento che si avr√† nell'objective function se quell'oggetto viene eliminato. Questo pu√≤ essere molto costoso.

##### Impact of Outlier on the Initial Cluster

Ci si pu√≤ porre la domanda "il clustering √® valido dopo aver determinato i suoi outlier?" dato che gli outlier vanno ad influenzare l'algoritmo di clustering. Per gestire questa problematica si pu√≤ rigenerare il clustering una volta rimossi gli outlier anche se questo non garantisce il miglioramento dei risultati. Un approccio piu sofisticato √® quello di generare un gruppo di potenziali outlier che verr√† popolato dai punti che non sono fortemente connessi agli altri mentre si effettua il clustering cos√¨ da poter essere eliminati direttamente. Anche questo metodo non garantisce un risultato ottimale o che funzioni meglio di quello pi√π semplice descritto prima.

##### Il numero di cluster da utilizzare

Un'altra problematica √® quella di determinare il numero di cluster poich√© pu√≤ far variare il processo di outlier detection. Per esempio, un numero elevato di cluster piccoli fornisce meno outlier che probabilmente sono pi√π veri rispetto a pochi cluster molto grandi. Un approccio per risolvere questo problema √® quello di ripetere pi√π volte l'analisi con differenti numeri di cluster oppure provare a trovare un grande numero di piccoli cluster perch√© piccoli cluster tendono ad essere pi√π coesi e perch√© se un oggetto √® un outlier con un grande numero di piccoli cluster allora √® pi√π probabile che sia un vero outlier.

##### Pro e Contro

- Alcune tecniche (come il K-means) hanno una complessit√† in spazio e tempo non lineare o lineare. Quelle con complessit√† lineari possono risultare molto efficienti
- Di solito si possono trovare contemporaneamente cluster e Outlier
- Dipendono molto dal tipo di algoritmo di Clustering e quindi possono essere estremamente influenzati dagli outlier (il caso dei prototype based)
- La bont√† degli outlier dipende fortemente dall'algoritmo di clustering scelto (dipendono fortemente dai tipi di dato)

#### Approccio Reconstruction Based

√à possibile ridurre il numero di features di un dato insieme di dati in maniera tale che queste features siano ancora rappresentative per i dati normali, ma non per le anomalie. Per i dati lineari √® possibile utilizzare la Principal Component Analysis (PCA), un approccio basto su combinazioni lineari degli attributi originali e altre trasformazioni strane dell'algebra lineare. Successivamente, dopo averle riportate alle dimensioni iniziali, sar√† possibile vedere se un dato √® anomalo in base a quanto si discosta dal valore originale. Questo prende il nome di reconstruction error ed √® definito come il quadrato della distanza euclidea.

![rec error](./imgs/recerror.gif)

dove $x$ √® il valore originale e $\hat{x}$ √® il valore ottenuto dalla ricostruzione.

Ci aspettiamo che il reconstruction error sia basso per dati appartenenti alla nostra distribuzione di dati, mentre risulti alto per dati anomali.

![rec error 2](./imgs/recerror2.png)

Nella foto precedente la linea nera rappresenta la direzione di massima varianza dell'istanza normale (come i dati vengono rappresentati una volta ricostruiti), le linee tratteggiate rappresentano il reconstruction error, i cerchi sono i dati normali, i quadrati neri gli outlier.

Per i dati non lineari non √® possibile applicare la PCA ed √® necessario utilizzare un approccio basato su MNN chiamato Autoencoder. Un autoencoder √® un MNN avente un numero di neuroni di input e di output uguali al numero di attributi originali, la sua architettura √® composta di due step principali:

- Encoding: riduce sempre di pi√π il numero di dimensioni delle feature utilizzando trasformazioni non lineari
- Decoding: mappa le rappresentazioni ottenute con l'encoding con lo spazio degli attributi originali ottenendo cos√¨ una ricostruzione di $x$ chiamata $\hat{x}$. La distanza tra questi 2 valori sar√† il reconstruction error, ovvero l'indice per l'anomaly detection. Il punto centrale di minori dimensioni viene chiamato bottleneck.

Esistono vari tipi di autoencoder come ad esempio il Denoising Autoencoder che √® in grado di apprendere rappresentazioni non lineari anche in presenza di rumore.

![autoencoder](./imgs/eutoencoder.png)

##### Pro e Contro

- Possono apprendere la rappresentazione di molte classi normali utilizzando svariate tecniche di Dimensionality Reduction
- Possono essere utilizzate anche in presenza di attributi irrilevanti (verranno ignorati nello step di encoding)
- Scarse performance quando il numero di attributi √® grande (a causa del calcolo del reconstruction error)

#### Approccio One Class SVM

Si possono utilizzare i classificatori per risolvere problemi di anomaly detection trasformandoli in un One Class Problem: un problema in cui l'interesse √® solo quello di determinare un decision boundary che rappresenti la classe normale.

![one class](./imgs/oneclass.png)

In questo caso possiamo utilizzare le SVM che riescono bene a trovare un boundary per effettuare questa distinzione. Come per il normale caso di SVM in ambito non lineare andremo ad utilizzare un kernel per trasformare i dati in una dimensione maggiore per trovare un iperpiano che li separa. Un kernel molto utilizzato √® quello Gaussiano che mappa i dati su una ipersfera di raggio 1 e tutti i punti sono sulla stessa orthant (l'equivalente del quadrante in pi√π di 2 dimensioni). Quindi andremo a trovare l'iperpiano che li separa meglio.
Un iperparametro molto importante √® $\nu$ (nu) che indica la percentuale di outlier che andremo a permettere. Questo fa si che nel nostro dataset possono essere presenti anche punti di outlier, a differenza degli autoencoder.

Queste SVM riescono a trovare boundary molto interessanti come le seguenti:

![decision](./imgs/decisionsvm.png)

##### Pro e Contro

- Forte base teorica (cosa buona perch√© sappiamo che funziona bene e perch√©)
- La scelta di $\nu$ √® molto difficile (va scelto bene)
- Risultano computazionalmente costosi per dati a tante dimensioni
- Ammettono punti di outlier nel dataset di training
- Molto efficaci per dataset di piccole dimensioni

#### Approccio Information Theoretic

Questo approccio codifica i dati e invece di apprendere la loro rappresentazione, basa la sua analisi sul quantitativo di informazioni che questi dati rappresentano. Le anomalie, poich√© sono irregolari rispetto ai dati, aumentano la quantit√† di informazioni del dataset. Ci sono vari approcci a seconda del tipo di dato, per dati qualitativi si pu√≤ utilizzare l'entropy, per i dati quantitativi si pu√≤ usare la Kolmogorov (Karasni) complexity. Un approccio pratico √® quello di comprimere i dati e rimuovere volta per volta dati per vedere se l'information gain aumenta. Nel processo di rimozione non si pu√≤ rimuovere un singolo dato alla volta perch√© le variazioni nell'information gain causate dalla rimozione di un singolo dato sono irrilevanti, dunque bisogna trovare il sottoinsieme X pi√π piccolo del dataset che mostra la pi√π grande variazione di information gai una volta eliminato.

##### Pro e Contro

- Sono versatili perch√© non fanno alcun tipo di assunzioni sulla struttura del dataset
- Non richiedono training
- La loro performance dipende pesantemente dalla misura scelta per calcolare le informazioni
- Sono computazionalmente costosi e difficilmente applicabili a dataset di grandi dimensioni

### Valutazione dell'Anomaly Detection

- Se sono presenti le class label nel dataset, allora si utilizzano gli approcci standard per la classificazione di classi rare: precision, recall, false positive rate (False alarm rate)
- Se non sono presenti label (unsupervised) si utilizzano misure fornite dal metodo di anomaly detection utilizzato: Reconstruction error o Information Gain 
- Si pu√≤ anche guarda la distribuzione degli anomaly scores con un istogramma o density plot per vedere se abbiamo dei risultati ragionevoli (se tutto √® un anomalia c'√® qualcosa che non va)

## Dimensionality Reduction

Spesso i dati hanno un numero estremamente alto di attributi che ne rendono la rappresentazione e complessa dunque per aumentare l'efficienza degli algoritmi di data mining spesso vengono applicate tecniche di dimensionality reduction che trasformano il dataset in un altro con un numero inferiore di features che vengono generate tramite combinazioni lineari delle features originali. Questo processo ha altri vantaggi come ad esempio la riduzione del numero di attributi irrilevanti o del rumore. Riesce anche a ridurre la Curse of Dimensionality :skull_and_crossbones:.

### Curse of Dimensionality ‚ò†Ô∏è

Si riferisce al fenomeno che rende i dati ad un elevato numero di dimensioni (attributi) difficilmente classificabili. Questo avviene perch√© se le dimensioni aumentano i dati diventano incrementalmente sparsi nello spazio che occupano e quindi √® possibile che i nostri data object non saranno un campione rappresentativo di tutti i possibili oggetti. Per la classificazione questo significa che non saremmo in grado di creare un modello affidabile e per il clustering significa che i concetti critici per la creazione di un clustering, come densit√† e distanza, diventano meno significativi.

![curse](./imgs/curse.png)

### Feature Selection

Un possibile approccio per ridurre il numero di dimensioni √® quello di selezionare un sotto set di attributi. Una possibile idea per svolgere questo compito potrebbe essere quella di testare tutte le possibili combinazioni di attributi con l'algoritmo bersaglio ma per $d$ attributi, verrebbero fuori $2^d$ sotto set da controllare che nella maggior parte dei casi √® un calcolo ingestibile. √à possibile applicare altre tecniche come:

- **forward selection**: inizi con un set di features vuoto e aggiungi ripetutamente le feature che riducono maggiormente l'errore fino a quando questi decrementi sono insignificanti (mean square error, misclassification error, ecc).
- **backward selection**: iniziamo con tutte le features e si rimuove la feature che decrementa maggiormente l'errore e si continua fin quando l'incremento di errore della rimozione √® molto significante.

Entrambi questi approcci hanno costo $O(d^2)$.

### Feature Extraction

Cerca di trovare un insieme di nuove features mappate tramite una data funzione. Spesso le combinazioni lineari si prestano bene a questo approccio perch√© sono semplici da calcolare e sono analiticamente trattabili.

![extraction](./imgs/extraction.png)

In base alla loro objective, si possono classificare in varie categorie per esempio:

- Minimizing information loss: rappresenta i dati nel modo pi√π accurato possibile in uno spazio a meno dimensioni (PCA)
- Maximize discriminatory information: accentua le informazioni determinanti per la classificazione in una spazio a meno dimensioni (utile per la classificazione)

Ma ce ne sono molte altre.

#### Tecniche Lineari

L'approccio pi√π comunemente utilizzato √® quello del Principal Component Analysis (PCA) che cerca una proiezione che preserva il maggior numero di informazioni possibili. Altri metodi sono:

- Linear Discriminant Analysis (LDA): cerca una proiezione che discrimina al meglio i dati
- Independent Component Analysis (ICA): rende le features il pi√π indipendenti possibile

#### PCA

Questa tecnica funziona proiettando i dati di input su gli eigenvector della matrice di covarianza dei dati.

1. Si calcola la matrice di covarianza `C` che serve a quantificare la varianza dei dati e quanto una variabile varia rispetto ad un'altra. ![covarianza](./imgs/covarianza.png)
2. Si trovano gli eigenvector `u_i` di `C`: ![culu](./imgs/culu.png)
3. Si cercano i `K` eigenvector pi√π grandi che corrispondono ai `K` eigenvalue pi√π grandi (`<u1, u2, ..., uk>`). Questa sar√† la nuova base del nostro spazio 

Stiamo essenzialmente estraendo i componenti di ogni variabile che porta alla maggiore varianza quando proiettiamo i dati su questi vettori. Usiamo gli eigenvalue della matrice di covarianza perch√© riflettono la magnitudine della varianza nella direzione dell'eigenvector corrispondente.

Per scegliere la dimensione `K` si utilizzano una threshold scelto arbitrariamente `T` che rappresenta la percentuale dell'informazione che vogliamo preservare. Se `K = N` verranno mantenute il 100% delle informazioni ed √® solo un cambio di base. Applicare solo un cambio di base potrebbe essere utile per rappresentare e visualizzare meglio i dati. 

![k](./imgs/k.png)

##### Pro e Contro

- Interpretabile
- Veloce nell'esecuzione
- Trova solo trasformazioni lineari
- Problema del Crowding
- La direzione della massima varianza non √® detto che sia la pi√π informativa
- Fallisce su dati composti da molteplici cluster separati

#### Crowding Problem

Il Crowding problem si presenta quando, passando da una dimensione pi√π grande ad una pi√π piccola, vogliamo preservare le distanze tra i vicini ma alcune volte risulta essere impossibile.

![crowding](./imgs/crowding.png)

Dalla foto sopra possiamo vedere che la distanza tra i vicini di $x_1$ non viene rispettata quando si riduce il numero di dimensioni.

#### t-SNE

t-distributed Stochastic Neighbor Embedding √® una tecnica di dimensionality reduction che si presta molto bene per la visualizzazione dei dati che prova a concentrare i punti con similarit√† maggiore il pi√π vicino possibile nello spazio a dimensioni minori (tenta di risolvere il crowding problem). Preserva la struttura locale dei dati utilizzando la distribuzione t-student.

L'algoritmo funziona nel nel seguente modo:

1. Cerca la pairwise similarity (similarit√† di coppia) tra punti vicini dello spazio ad alte dimensioni (definita in termini di probabilit√† di essere vicini)
2. Mappa ogni punti nello spazio a meno dimensioni basandosi sulla pairwise similarity
3. Cerca una rappresentazione nello spazio a meno dimensioni che minimizza le differenze tra i dati. Lo fa utilizzando il gradient descent e il KL-divergence (KullbakLaber divergence)

![tsne](./imgs/tsne.gif)

##### Pro e Contro

- √à ottimo per visualizzare i dati
- Aiuta a comprendere gli algoritmi black box come DNN
- Riduce il problema del Crowding con distribuzioni heavily tailed
- Non convesso, quindi richiede gradient descend con momentum

### Representation Learning (aka Features Learning)

Questo tipo di learning basa il suo principio di funzionamento sull'apprendere una rappresentazione dei dati di input che potranno poi essere passati a vari modelli che riescono a gestire tale rappresentazione. Si presta bene a Unsupervised e SemiSupervised learning perch√© permette di combinare dataset con label e dataset senza label insieme per ridurre l'overfitting che potrebbe derivare dall'utilizzo del solo dataset con label. Questo perch√© l'algoritmo studia features sul dataset senza label ed impara come classificarle utilizzando il dataset con label.

In sostanza, rende input che potrebbero essere difficili da utilizzare per algoritmi di ML facilmente rappresentabili ed utilizzabili.

#### Natural Language Processing üóª ü™µ

NLP √® il campo che si occupa della comprensione del processing, dell'analisi e della generazione dei linguaggi naturali (CARPI TRIGGERED).
Pu√≤ essere applicata a vari campi come:

- TRANSlation
- Information Extraction
- Summarization
- Parsing
- Question answering
- Sentiment Analysis ‚ù§Ô∏è
- Text Classification

NPL √® un campo molto difficile che presenta svariati problemi da risolvere ma negli ultimi anni linguisti e informatici ci si sono dedicati moltissimo ed hanno ottenuto risultati molto interessanti. Questi risultati non sono quelli ottimali, ma risultano essere molto buoni e soprattutto estremamente migliori rispetto a quelli ottenuti 10 anni fa.

Gli aspetti pi√π complessi dell'NLP sono i seguenti:

- **Polysemy**: a seconda di dove vengono utilizzate le parole possono assumere diversi significati. `Book a flight` e `read a book`, `book` come prenotare o libro. 
- **Syntactic Ambiguity**: la sintassi della frase pu√≤ essere interpretata in modi diversi. `Kids make nutrition snacks` si interpreta come `I bambini sono degli snack nutrienti` (come mangiare i bambini) o `I bambini fanno degli snack nutrienti` (come li preparano).
- **Variability**: frasi con diverse parole ma che assumono lo stesso significato. `They allowed him to ...` e `They let him ...`
- **Co-reference Resolution**: trovare i nomi a cui i pronomi fanno riferimento. ![esempio coreference](./imgs/coreference.png)
- **Carenza** di dati o quantitativi **enormi** di dati

##### World Embedding

La rappresentazione delle parole √® semplice per l'uomo e non per la macchina dunque √® necessario studiare un metodo per rendere questa rappresentazione utilizzabile nei processi di allenamento di ML. A questo scopo √® stato studiato il World Embedding in cui le parole vengono rappresentate come vettori a valori reali che codificano il significato delle parole in maniera tale che le parole pi√π vicine nello spazio vettoriale ci si aspetta che saranno simili nel significato. Si utilizza anche il contesto in cui la parola viene utilizzati per capirne il significato. Il Word Embedding si pu√≤ ottenere utilizzando un insieme di tecniche di language modeling e feature learning.

Viene utilizzato per vari problemi di NLP come:

- Semantic Similarity
- Word Sense Disambiguation
- Semantic Role (Playing) Labeling
- Named Entity Recognition üëª
- Summarization
- Question Answering
- Sentiment Analysis ‚ù§Ô∏è

I modelli pi√π famosi per World Embedding sono (vengono visti come blackbox, non ci serve sapere i dettagli, li usiamo e basta):

- World2Vec (2013)
- GloVe (2014)
- Fast Text (2017)

###### Word2Vec

Questa √® la tecnica pi√π giovane e la prima ad esplorare l'utilizzo delle Neural Network in modo efficacie per generare l'embedding. L'implementazione si basa sulla seguente intuizione: due parole che condividono un contesto simile sono associate con vettori vicini tra loro nello spazio vettoriale. Non siamo interessati all'output di questi modelli ma solo ai pesi appresi durante il learning process perch√© saranno quelli che faranno il nostro embedding. Un po' come gli Autoencoder che, come side effect, hanno la features reduction, anche questi come side effect hanno l'embedding. In pratica ci sono 2 diversi modelli:

- CBOW üêÑ: prova a prendere un insieme di parole e predirre quella mancante
- Skip Gram: predice le parole in base al contesto dell'input

Tutti e 2 i modelli come dataset utilizzano una enorme collezioni di testi grammaticalmente corretti del linguaggio scelto che prende il nome di `corpus`. La scelta dei testi √® molto importante ed deve basarsi sull'obbiettivo finale dell'embedding: per fare l'embedding dei paper che trattano argomenti scientifici non si potranno utilizzare articoli di un forum di cucina. L'idea √® che non devo insegnare al modello come funziona la grammatica ma la apprende da solo.

**Continuos Bag of Words (CBOW) üêÑ**

![cbow](./imgs/cbow.png)

Una parola pu√≤ essere rappresentata da un tot di parole che la precedono e un tot che la seguono, questo √® il contesto. Nella foto possiamo vede che la parola viene rappresentata come le 2 precedenti $w_{t-1}$ e $w_{t-2}$ e le due successive $w_{t+1}$ e $w_{t+2}$. In pratica abbiamo un modello con un solo hidden layer, che generalmente utilizza la cross entropy loss, e creiamo un problema di apprendimento supervisionato basandoci sul testo che abbiamo (la label dell'output sar√† la parola target). Successivamente alleno la rete per taaaantissime epoche e alla fine prendo in considerazione solo i pesi ottenuti durante questa fase per creare l'embedding matrix che utilizzer√≤ per il mio problema di NLP.

N.B.: nella foto, il numero 300 indica la dimensione del feature vector che rappresenta una parola e 100k il numero di parole che formano il dataset (corpus). Avr√≤ quindi una matrice finale 100kx300. 

**Skip Gram**

![skipgram](./imgs/skipgram.png)

Si basa sullo stesso concetto di CBOW ma √® il contrario: partiamo da una parola centrale e vogliamo trovare il contesto (le 2 parole successive e le 2 precedenti). Come per l'altro modello andiamo a creare un learning task supervisionato e alla fine dell'allenamento prendo i pesi e ignoro l'output (esattamente come l'altro).

**Modelli Preallenati**

Quando utilizziamo modelli pre allenati (cosa abbastanza comune) bisogna prestare attenzione ai parametri del modello che possono andare ad alterarne le performance. Parametri importanti sono:

- window size: √® il numero di parole che compongono il contesto (tipo $w_{t-1}, w_{t-2}, ecc.$)
- il corpus
- numero di iterazioni
- dimensione del feature vector

**Rappresentazione**

Una volta ottenuto l'embedding possiamo anche rappresentare le parole in uno spazio 2D per poterle visualizzare meglio. √à da notare che 2 (che viene dal 2D) non √® il numero di features utilizzate per rappresentare le parole ma il tutto √® il risultato di applicazione di un visualization algorithm tipo il t-sne.

![parole grafo](./imgs/parolegrafo.png)

Si √® anche visto che √® possibile definire una specie di algebra con queste parole, di seguito possiamo apprezzarne un esempio:

![parole](./imgs/algebra.png)

√à anche possibile risolvere delle proporzioni del tipo `roma: italia = parigi : francia` e quindi `roma : italia = x : cuba`.

> in italiano we say proporzioni.

## Recurrent Neural Networks

In alcune applicazioni del machine learning √® importante tenere in considerazione la sequenza in cui si presentano i dati in input, questo perch√© sono informazioni che risultano significative per ottenere il corretto output (andamento del mercato che dipende dai prezzi precedenti o text processing che dipende dalla semantica). Nelle architetture standard tuttavia non si pu√≤ tenere conto di questa sequenzialit√† dato che trattano ogni input come indipendente l'uno dall'altro (tipo i pixel di una foto). Per questo motivo sono state studiate le Recurrent Neural Networks che effettuano parameter sharing tra i vari layer della rete e riuscendo cos√¨ a tenere in considerazione la sequenza in cui si presentano i dati. Spesso sono anche in grado di prendere in input sequenze di dimensioni variabili. Esempi possibili sono:

- Predizione di testo
- Traduzione 
- Image Captioning
- Sentiment Analysis
- Sentence Centric Classification

Due esempi di architetture di RNN sono:

- LSTM, Long Short Term Memory
- GRU, Gayted Recurrent Unit

Le RNN sono _TURING COMPLETE ‚ößÔ∏è_, con abbastanza tempo e risorse pu√≤ simulare qualsiasi algoritmo. Questa informazione √® completamente inutile perch√© per riuscire in questo compito servono risorse e tempo estremamente elevate che a volte potrebbero essere anche irrealistiche. Spesso un altro problema in cui incappano questi modelli √® quello del gradient vanishing e exploding la cui severit√† aumenta all'aumentare della lunghezza della sequenza in input.

![alan gay](./imgs/analgay.gif)

### Struttura Base

Per semplificare la visualizzazione di queste reti si pu√≤ introdurre l'idea dell'Unfolding Computational Graph, con cui si dispiegano una serie di calcoli ricorsivi in un grafo che ha una struttura ripetitiva tipicamente corrispondente ad una catena di eventi. Questo grafo inoltre mostra la condivisione dei parametri lungo la DNN.

![unfolding](./imgs/unfolding.png)

Ora che abbiamo introdotto il concetto di unfolding possiamo analizzare le varie metodologie di design delle RNN.

I design patter pi√π importanti sono i seguenti:

- Recurrent Network che producono un output ad ogni time-step ed hanno connessioni ricorrenti tra le hidden unit ![rnn](./imgs/rnn.png)
- Recurrent Network che producono un output ad ogni time-step ed hanno recurrent connection solo tra output ad un time step e hidden unit al time step successivo. Questo design sar√† sempre peggiore degli altri e non √® turing completo perch√© le informazioni non vengono passate direttamente tra hidden layer ma tra output ed hidden layer poich√© l'output layer prova a far combaciare con la propria matrice dei pesi l'output con la ground through sar√† difficile preservare le informazioni. Il vantaggio di togliere hidden to hidden connection √® che il calcolo del gradiente per ogni time stamp pu√≤ essere parallelizzato ![rrn out h](./imgs/rnnouth.png)
- Recurrent Network con un solo output finale con connessioni ricorrenti tra hidden units. ![rnn fine](./imgs/rnnoutfine.png)

Nella forward phase abbiamo le seguenti equazioni:

- ![ht](./imgs/ht.png)
- ![altre](./imgs/altrernnform.png)

Dove $\sigma$ √® la funzione di attivazione per l'hidden layer che di solito √® la `tanh`.

I parametri allenabili della rete sono: 

- `b` e `c`: rappresentano i vettori di bias
- `U`, `V`, `W`: sono le matrici dei pesi, che rappresentano rispettivamente le connessioni _input-to-hidden (U)_, _hidden-to-output (V)_, _hidden-to-hidden (W)_

$\hat{y}$ √® la probabilit√† di output normalizzata che servir√† per il confronto, tramite Loss Function, con la ground through $y$; $o_t$ √® l'output non normalizzato che verr√† utilizzato per calcolare $\hat{y}$ tramite una funzione di attivazione (di solito la softmax).

Calcolare il gradiente √® costoso e richiede svariati passaggi, inoltra non pu√≤ essere parallelizzato e dunque il tempo √® `O(T)` (dove `T` √® il numero di time step). Dato che stati calcolati nella fase forward devono essere salvati avremo un costo in memoria equivalente. Questo approccio √® chiamato Back Propagation Through Time (BPTT).

#### Sequence To Sequence Learning

A seconda della mappatura tra input e output possono essere realizzate delle reti che saranno utili per varie situazioni:

- One to One: semplice rete feed forward, utile per l'image classification
- One to Many: utile per image captioning
- Many to One: utile per text classification e sentiment analysis
- Many to Many: utile per machine translation
- Sync Many to Many: utile per video classification (image classification per ogni frame)

![sequenctetose](./imgs/sqeunecetosence.png)

### BPTT

Nella fase di apprendimento, per aggiornare le matrici di pesi non possiamo utilizzare la normale back propagation perch√© la funzione $h_t()$ dipende da $h_{t-1}()$ (e cos√¨ via fino all'inizio) e non risultano derivabili con la normale back propagation (causerebbe anche perdita di informazioni). Si utilizza una versione leggermente diversa chiamata Back Propagation Thought Time perch√© sa tornare "indietro nel tempo".
Vedremo una versione molto semplificata dei calcoli.

L'idea √® che nella la fase forward, andiamo a calcolare la Loss Totale (calcoliamo ogni $Loss_i$ e le sommiamo insieme) e da questa possiamo calcolare il gradiente di ogni parametro per andarlo poi ad aggiornare nella fase backward. 

<img src="./imgs/losstotale.png" width="400">

_Dove_ $L_i$ _√® la loss function del time step_ $i$

Queste sono le formule con cui vengono aggiornate le matrici di pesi ad ogni epoca in base alla loss function.
![formule](./imgs/formule.png)

Prendiamo il caso di `V`, con pochi calcoli possiamo vedere che il calcolo del gradiente pu√≤ essere riportato alla seguente sommatoria: ![calcoli](./imgs/caclolibptt.png)

Tutti i prodotti tra quelle derivate sono date dalla chian rule che serve per rendere derivabili funzioni composte, dato che $L_i$ dipende da $\hat{y}$ che dipende da `z`. Questi calcoli non sono problematici per il computer e si fa tutto senza tanti problemi.

Il vero problem viene con `W` e `U`.
Prendiamo in esame `W`, la cui formula per calcolare il gradiente √® la seguente: ![doppiaw](./imgs/doppiaw.png)

Dobbiamo quindi calcolare tutte quelle derivate e sommarle. Per L1 no ci sono molti problemi dato che h1 dipende da h0 (che praticamente √® una costante). Il problema sta nel calcolo di quelle successive, perch√© bisogna ripercorrere i vari time step precedenti fino ad arrivare ad h0 (qui prende il nome di BP through time perch√© torna "indietro nel tempo"). Di seguito un esempio di come effettivamente si torna indietro nel tempo per il calcolo della seconda derivata.

![](./imgs/bpttgraph.png)

Possiamo generalizzare tutti questi calcoli in una formula pi√π compatta: ![produttoria2](./imgs/produttoria2.png). 
Qui sorgono due problemi, il primo √® che questi calcoli risultano mooolto complessi e numerosi piu andiamo avanti nel tempo (il time step 20 dipende dal 19 che dipende dal 18, e cos√¨ via) il secondo √® che quella produttoria causa il vanishing/exploding del gradiente (moltiplicare tante volte la stessa cosa o la porta all'infinito o a 0).

Lo stesso problema avviene per `U` dato che anche qui per sviluppare i calcoli dobbiamo passare per `h`.

In caso guarda questo video: [Mega Indi che spiega cose](https://www.youtube.com/watch?v=phOVApJHjsU&list=LL&index=1&ab_channel=AhladKumar)

### Deep Recurrent Networks

Fino ad ora abbiamo visto delle reti ricorrenti formate da un solo hidden layer, ma aggiungendone altri adiamo ad avere dei miglioramenti nelle performance significativi ?

Da degli studi pratici √® emerso che questa aggiunta √® sensata e apporta miglioramenti nelle performance della nostra rete. 

Esistono 3 modi per farlo:

- **Hidden Recurrent Layers**: aggiungiamo pi√π layer ricorrenti tra l'input e l'output. Questo fa si che i layer pi√π vicino all'input riescono a trasformarlo in modo da rendere la rappresentazione dei dati pi√π appropriata per i layer successivi ![hiddenrec](./imgs/hyddenrec.png)

- **Hidden MLP/Pompotron layers**: possiamo aggiungere layer MLP in qualunque punto della rete (input-to-hidden, hidden-to-hidden, hidden-to-output). Questo aumenta la capacit√† (memoria) della rete ma, aggiungendo profondit√†, va a peggiorare in termini di tempo e risorse il processo di learning e ottimizzazione. Questo avviane perch√© i MLP layer aumentano la distanza tra il time step `t` e il time step `t + 1` (solitamente la raddoppiano). ![hidden mlp](./imgs/mlprec.png)

- **Hidden MLP/Pompotron + Skip Layers**: per evitare il problema dell'allungamento della distanza tra i time step (problema del punto precedente) possiamo introdurre, oltre che layer MLP, anche skip layer per accorciare queste distanze. ![skipmlp](./imgs/mlpskip.png)

In generale √® pi√π semplice ottimizzare e lavorare con architetture pi√π semplici e meno profonde possibili.

### Il problema delle Long-Term Dependencies

Un problema molto importante dell RNN √® quello delle Long-Term Dependencies ovvero la propagazione del gradiente per molti step (quindi passare per moooolti hidden layer recurrent, anche solamente 10 o 20), tende a risultare in un vanishing o exploding del gradiente. Da un punto di vista matematico, il problema risiede nel moltiplicare la matrice dei pesi hidden `W` tantissime volte per se stessa (alla fine viene fuori $W^t$). QUesta moltiplicazione di matrici deriva dal fatto che la funzione $h_t$ dipende da $h_{t-1}$ e continua cos√¨ ricorsivamente. Si possono fare calcoli strani per scriver questa matrice come prodotto di eigenvalues e eigenvectors(??). Moltiplicando moooolte volte eigenvalues con base (la chiama magnitude) < 1 il calcolo tende a diventare 0, quelli con magnitude > 1 tendono ad esplodere (infinito).
Con questo problema la fase di learning pu√≤ impiegare tantissimo tempo ad apprendere queste Long-Term Dependencies o non riuscirci affatto.

Sono stati quindi introdotti alcuni metodi per cercare di risolvere questo problema.

### Gestire le Long-Term Dependencies

Un modo per gestire le long-term dependencies √® quello di creare un modello che opera a molteplici time scale in maniera tale che alcune parti del modello opereranno con un time scale a grana fine e gestiranno piccoli dettagli mentre altre parti opereranno a time scale a grana grossa passando informazioni da passati distanti al presente in maniera efficiente. Le strategie piu importanti per gestire questo problema sono quelle delle:

- Skip Connection: aggiungono collegamenti tra variabili in un passato lontano a variabili nel presente
- Leaky Units: integrano segnali con differenti costanti di tempo e rimuovono alcune connessioni utilizzate per modellare i time scale a grana fine

#### Skip Connection

Questa tecnica √® stata introdotta per cercare di ridurre il problema del gradient vanishing/exploding introducendo delle connessioni dirette che passano da un time step `t` ad un time step `t + d` riducendo quindi il numero di dipendenze per tutti gli hidden layer sopra a `t + d` (da `t+d` pu√≤ tornare indietro direttamente a `t` senza passare per tutti gli altri). In questo modo il gradiente non diminuir√† esponenzialmente in funzione di `T` ma in funzione di `T/d` rallentandone quindi il vanishing/exploding (pu√≤ comunque esplodere/scomparire). Questo permette all'algoritmo di learning di rappresentare delle dipendenze piu lunghe che per√≤ non √® detto che verranno rappresentate bene.

![skip the portal](./imgs/skiptheportal.png)

#### Leaky Units

L'effetto di skip di `d` time step (con `d` un numero intero) pu√≤ anche essere ottenuto con un numero reale `a` che essendo reale permette aggiustamenti pi√π fini e precisi. Le leaky units sono hidden units cone self connection lineari. Quando `a` √® vicino ad 1 il modello ricorda informazioni del passato per un lungo tempo mentre quando √® pi√π vicino a 0 le informazioni vengono scartate rapidamente.

#### Gated RNN :family_man_man_girl_boy:

Ad oggi le Gated RNN sono il metodo pi√π utilizzato nella pratica per realizzare RNN. Come le Leaky Unit il loro funzionamento si basa sull'idea della creazione di cammini attraverso il tempo le cui derivate non svaniscano o esplodano. Mentre nelle leaky unit queste weighted connection venivano scelte manualmente le gayted RNN provano a generalizzare, permettendo al modello di apprendere quale sia il migliore valore da assegnargli (ricordiamo che per valori vicino ad 1 si ha una memoria a lungo termine, per valori vicino a 0 si indurr√† il modello a dimenticare il passato). Quindi il modello sar√† in grado di decidere automaticamente quando dimenticare eventi passati impostando lo stato a 0.

Due implementazioni di Gayted RNN sono:

- Long Short-Term Memory (LSTM) [BDSM]
- Gayted Recurrent Unit (GRU)

#### LSTM

Per permettere al modello di aggiustare i parametri di queste connessioni tra i vari time step viene introdotta un'unit√† che andr√† a sostituire il classico hidden state visto fin ora. Queste unit√† sono composte internamente da 3 gate che prendono in input l'hidden state precedente e la sequenza in input attuale. Queste gate sono:

- Input Gate
- Output Gate
- Forget Gate

Esternamente si comportano come un hidden state visto fin ora quindi si collegano anche con l'hidden state successivo.

![lstm](./imgs/lstm.png)
_Rappresentazione interna di una cella_

> LARGER IS BETTER
>  _Valentina Poggioni_

#### GRU

√à una semplificazione di LSTM che combina l'input e il forget gate in un unico update gate e unisce in una sola cella la Cell State e l'hidden state. Queste stanno diventando sempre pi√π popolari per via della loro semplicit√† e perch√© sono pi√π semplici da allenare.

#### Gradient Clipping

A causa delle numerose moltiplicazioni della stessa matrice di pesi si possono forma delle "colline": una regione di pianura seguita ta una discesa ripida e poi una sezione di pianura. Quando il calcolo del gradiente approccia questa collina pu√≤ succedere che i parametri vengono lanciati molto lontano causando la perdita di tutta l'ottimizzazione fatta fin ora. Per risolver questo problema una tecnica √® quella del Gradient Clipping che pu√≤ essere applicata in 2 modi:

- clippare il gradiente, prima che i parametri vengano aggiornati, all'interno di un minibach element-wise (dovrebbe essere un area oltre la quale il gradiente non pu√≤ andare)
- clippare il gradiente se la sua norma supera una certa soglia `v` ![clip form](./imgs/clippingforuma.png)

![clppng](./imgs/gradientclipping.png)